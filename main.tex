\documentclass[12pt,a4paper]{report}
\usepackage{Bath-CS-Dissertation}
\usepackage[newfloat]{minted}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usemintedstyle{vs}

\title{\bf $\langle$Dissertation Title$\rangle$}
\author{Jay Rabjohns}
\date{Bachelor of Science in Computer Science\\ 
      The University of Bath\\
      2024}

% Numberings for Listing
\makeatletter
%\renewcommand*{\thelisting}{\thesection.\arabic{listing}}
\renewcommand*{\thelisting}{\thechapter.\arabic{listing}}
\@addtoreset{listing}{section}
\makeatother

% Numberings for theorems, corollaries, and lemmas
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]%section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\begin{document}
\hypersetup{pageanchor=false}

% Set this to the language you want to use in your code listings (if any)
\lstset{language=Haskell,breaklines,breakatwhitespace,basicstyle=\small}

\setcounter{page}{0}
\pagenumbering{roman}

\maketitle
\newpage

\declaration{$\langle$Dissertation Title$\rangle$}{Jay Rabjohns}
\newpage

\hypersetup{pageanchor=true}
\abstract
$\langle$The abstract should appear here. An abstract is a short paragraph describing the aims of the project, what was achieved and what contributions it has made.$\rangle$
\newpage

\tableofcontents
\newpage

%\listoffigures
%\newpage

%\listoftables
%\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Acknowledgements}
Add any acknowledgements here.

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
This is the introductory chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature and Technology Survey}
This chapter discusses the literary and historical background that the project relies on. It starts by introducing concepts and discussing their historical significance before ultimately landing on concrete implementation ideas for the project at hand. Some of the topics include:
\begin{itemize}
    \item The $\lambda$-calculus and its computational model
    \item SPCF and other extensions of the $\lambda$-calculus
    \item Nesting removal in SPCF and its theoretical base
    \item Implementation details for an SPCF interpreter
\end{itemize}

\section{The \texorpdfstring{$\lambda$}{lambda}-calculus}
The lambda calculus is an important tool in the field of functional programming. It is an abstract model of computation introduced by \cite{church_1936} which provides compact semantics for studying computation. It is analogous to a simple yet very powerful programming language. While it was originally used to study the foundations of mathematics, specifically the 'Entscheidungsproblem' or 'Decision Problem', it has since been adapted and expanded to accommodate a wide array of domains, including being the basis for functional programming as a whole. It is computationally complete, meaning it can represent any computable function or equivalently it can simulate any Turing machine \cite{turing_1937}.

The lambda calculus is defined by the BNF grammar \eqref{eq:lambda_calc}, which provides an inductive definition for all lambda terms. Each term denotes a function and any term can be applied to any other term. $x$ is one of infinitely many variables, represented as a string. $\lambda x.N$ denotes an abstraction, which are functions that evaluate $M$ by binding all free occurrences of the supplied argument $x$ in $M$. They can be thought of as the suspended execution of a function, allowing it to be composed and reasoned about before its evaluation. $MN$ is the application of an argument $N$ to a function $M$. Together, these provide the basis to construct any valid term in the lambda calculus, highlighting how concise of a definition it has and further making the fact it is Turing complete quite incredible.

\begin{equation}\label{eq:lambda_calc}
    M,N ::= x\ |\ \lambda x.M\ |\ MN
\end{equation}
\subsection{Computation in the \texorpdfstring{$\lambda$}{lambda}-calculus}

Computations in the lambda calculus are usually presented as a series of transformations $M \rightarrow M' \rightarrow M'' \rightarrow \ldots$. The basic computation step is a $\beta$-reduction \eqref{eq:beta_reduction}, where a term $(\lambda x.M)N$ is said to reduce to $M[N/x]$. This means that every occurrence of $x$ in $M$ is substituted with $N$. This is an example of a reducible expression or $\beta$-redex. A normal form is a term to which no further computation can be performed, they are significant because ultimately they provide a convenient way of defining relations between terms. One takeaway from this which may not be immediately obvious is that terms can contain free variables, terms which reference variables not bound by a surrounding abstraction. This will become especially relevant later in Section \ref{sec:nesting_removal} when discussing argument sharing.

\begin{equation}\label{eq:beta_reduction}
   (\lambda x.M)N \rightarrow_{\beta} M[N/x]
\end{equation}

Relations can be defined between terms, denoted by $M \sim N$, which means that $M$ and $N$ are related by $\sim$. It is possible to define equivalence relations for expressing equivalence, which can be thought of as being identical modulo something related to the relation. $\alpha$-equivalence $M =_{\alpha} N$ states that terms are equivalent if they are identical in every respect modulo variable names. $\beta$-equivalence $M =_{\beta} N$ holds when $M \rightarrow_{\beta}^* N$, ensuring that each substitution $M[N/x] =_{\alpha} M$ to avoid variable capture. Probably the most important equivalence relation to the project is observational equivalence $M \simeq N$, where terms are considered to be equivalent if their outputs are indistinguishable for any given input, this will become more relevant in Section \ref{sec:nesting_removal}.

Many familiar higher-level constructs can be defined in the lambda calculus such as booleans, if-then-else expressions, natural numbers, and numerical operations like addition. These constructs allow real computation to be performed in terms of the lambda calculus. Church introduced a series of encodings for these, aptly named the Church encodings; \eqref{eq:booleans} provides definitions for booleans and $ifthen$. It is common for people to define a series of named constants as syntactic sugar for the lambda calculus, making programs considerably more readable while keeping the computational power. An example reduction of $ifthen$ can be seen below.
\begin{equation*}
\begin{split}
ifthen\ true\ M\ N &\rightarrow_{\beta}^* M\\
ifthen\ true\ M\ N &= (\lambda b. \lambda x.\lambda y.bxy)(\lambda x.\lambda y.x) M N\\
& \rightarrow_{\beta} (\lambda x. \lambda y.(\lambda x.\lambda y.x) x y) M N\\
& \rightarrow_{\beta} (\lambda y.(\lambda x.\lambda y.x) M y) N\\
& \rightarrow_{\beta} (\lambda x.\lambda y.x) M N\\
& \rightarrow_{\beta} (\lambda y.M) N\\
& \rightarrow_{\beta} M
\end{split}
\end{equation*}

Oppositely, it can be said that $ifthen\ false\ M\ N \rightarrow_{\beta}^* N$, but the full reduction is omitted here.

The encodings for numerals and associated operators have been omitted for brevity.
\begin{equation}\label{eq:booleans}
\begin{split}
true &= \lambda x.\lambda y.x\\
false &= \lambda x.\lambda y.y\\
ifthen &= \lambda b. \lambda x.\lambda y.bxy
\end{split}
\end{equation}

Recursion is the act of a function referencing itself. In the lambda calculus, recursion is modelled by a so-called 'fixed point' operator, defined by $M =_{\beta} F\ M$, where the input term of a function is equal to its output. Every term in the lambda calculus has at least one fixed point. The fixed point of a term can be found through a so-called 'fixed point combinator', which cleverly uses self-application to deduce the fixed point. Many fixed point combinators exist, one of the simplest is the Y-combinator \eqref{eq:y_combinator} introduced by \cite{curry_1930}. A combinator is simply a closed term, a term with no free variables. As an aside, the constants such as $ifthen$ and $true$ defined earlier are also kinds of combinator.

\begin{equation}\label{eq:y_combinator}
    Y = \lambda f.(\lambda x.f(xx))(\lambda x.f(xx))
\end{equation}

\subsection{Typing in the \texorpdfstring{$\lambda$}{lambda}-calculus}
It is possible and indeed common to extend the lambda calculus by editing its grammar. One such way is the addition of types, creating a typed lambda calculus. Types encode additional information about terms, including how they can be applied to one another. The addition of types is a trade-off, it cannot express all terms from the lambda calculus, however more can be proven about the terms it can express. Generality is lost in favour of specificity.

The simply typed lambda calculus is a form of typed lambda calculus originally introduced by \cite{church_1940} to address what he felt was a paradox in his untyped lambda calculus, self-application. An example of self-application is the term $\lambda x.xx$, commonly denoted $\Omega$, which is commonly used as an example of a diverging program. A grammar for the STLC is given by \eqref{eq:typed_lambda_calc}, where types are either a base type $o$ or constructed from two types $\sigma \rightarrow \tau$. Comparing the grammar to the untyped lambda calculus \eqref{eq:lambda_calc}, we see that the typing rules restrict the terms which can be applied to abstractions, illustrated by binding variables being annotated by a type $\tau$.

\begin{equation} \label{eq:typed_lambda_calc}
\begin{split}
    M,N &::=x\ |\ \lambda x^{\tau} .M\ |\ MN\\
    \tau , \sigma &::= o\ |\ \sigma \rightarrow \tau
\end{split}
\end{equation}

The significance of preventing self-application is that it is said to be strongly normalising, every term terminates and every term has a $\beta$-normal form, shown by \cite{tait_1967}. The trade-off for this is that recursion is no longer possible since self-application is not possible. This can be illustrated by trying to type the Y-combinator \eqref{eq:y_combinator}, where any type assigned to $x$ will always lead to a contradiction of the type for $f$. We will come back to this when reasoning about affine programs in section \ref{sec:nesting_removal}, specifically the differentiation between bounded and unbounded programs.

\section{Extensions of the simply typed \texorpdfstring{$\lambda$}{lambda}-calculus}
\subsection{Logic of Computable Functions}
In 1969 Dana Scott proposed the logic of computable functions (LCF), a logic which serves as the basis for a typed calculus supporting fixed-point recursion. It relies on the concept of a complete partial order (CPO), which is a relation between sets '$\leq$' that captures the idea of ordered computation. More specifically, elements of a set are related by a partial order and it is complete in the sense that each sequence of partially ordered elements has a least upper bound. In the context of fixed points, Scott shows that it is possible to construct a 'least fixed point', which is the smallest solution to a fixed-point equation, which implies that the recursion converges to a solution. Remember that while recursion via the least fixed point converges, it is not implied that the function terminates. For example, a function which always returns a constant value and never terminates is considered to have converged. LCF has laid the groundwork for more complex calculi than the STLC, and indeed Scott later used it to construct his X-Calculus but that is unrelated to the project at hand. This work was originally part of an unpublished note however it has since been published as a memorandum by \cite{milner_1973} to make it more accessible.

\subsection{Programming of Computable Functions}
PCF is a sequential functional language based on LCF introduced by \cite{plotkin_1977}. Whilst LCF is focused on providing a logic for proofs, PCF focuses more on practical computation and looks similar to a real-world programming language. It is, in essence, an extension of the simply typed lambda calculus supporting recursion as well as providing data types and functions out of the box. Its grammar is shown in \eqref{eq:pcf_grammar}, and similar to the STLC types are either a ground type or constructed between two existing types.

\begin{equation} \label{eq:pcf_grammar}
\begin{split}
    M,N &::= n\ |\ f\ |\ x_{\tau}\ |\ (\lambda x_{\sigma} .M_{\tau})_{\sigma \rightarrow \tau}\ |\ (M_{\sigma \rightarrow \tau} \ N_{\sigma})_{\tau}\\
    n &::= 1, 2, 3, \dots\\
    f &::= succ_{o \rightarrow o} \ | \ pred_{o \rightarrow o} \ | \ cond_{o \rightarrow o \rightarrow o \rightarrow o} \ | \ Y_{(\tau \rightarrow \tau) \rightarrow \tau}\\
    \tau , \sigma &::= o\ |\ \sigma \rightarrow \tau
\end{split}
\end{equation}

One of the major challenges relating to PCF has been to create a model which is fully abstract, a crucial property for characterising the observational equivalence of programs. \cite{plotkin_1977} admits that in order for the provided model to be fully abstract, there must exist functions capable of computing more than one argument simultaneously, which contradicts PCF being a sequential language. This is due to the CPO model of PCF being defined over continuous functions despite the language only being able to represent sequential functions. Further models have been developed which provide full abstraction, notably one by \cite{milner_1977}, however these are considered less than satisfactory. Later, \cite{loader_1996} disproved the existence of an \textit{effectively representable} fully abstract model of finitary PCF, which is another important property regarding the definition of observational equivalence. Another approach to the problem is to extend the language rather than to modify its model, enabling the definition of a sequential and fully abstract language. This is what will be discussed in the next section.

\subsection{Sequential PCF}\label{sec:spcf}
SPCF is an extension of PCF developed by \cite{cartwright_1992} which introduces error generators and escape handlers, acting as two kinds of control operators. Error generators describe the misapplication of terms and escape handlers can be thought of as escaping from local evaluation of a phrase.

\begin{equation} \label{eq:spcf_grammar}
\begin{split}
    M,N &:= n\ |\ f\ |\ e\ |\ x^{\tau}\ |\ (\lambda x^{\sigma} .M^{\tau})^{\sigma \rightarrow \tau}\ |\ (M^{\sigma \rightarrow \tau} \ N^{\sigma})^{\tau}\\
    n &= 1, 2, 3, \dots\\
    f &= succ^{o \rightarrow o} \ | \ pred^{o \rightarrow o} \ | \ if0^{o \rightarrow o \rightarrow o \rightarrow o} \ | \ Y^{(\tau \rightarrow \tau) \rightarrow \tau}\\
    e &= error_1^{\ o}\ |\ error_{2}^{\ o}\ |\ catch^{\tau_1 \rightarrow \dots \tau_n \rightarrow o}\\
\end{split}
\end{equation}

\begin{equation}
    \tau , \sigma := o\ |\ \sigma \rightarrow \tau
\end{equation}

\subsubsection{Observing evaluation order with errors}\label{section: observing sequentiality}
Functions in SPCF are error-sensitive, meaning that if an argument evaluates to an error the function also returns an error. This error propagation is analogous to the behaviour of $try...catch$ statements commonly found in practical languages. Error sensitivity allows a programmer to determine the evaluation order of a function's arguments by substituting them with distinct error values and observing which one is thrown. For example, consider two possible recursive definitions of addition in SPCF. One recurses on its first parameter and the other on its second.

\[+_l = Y(\lambda+.(\lambda x.\lambda y.\ if0\ x\ y\ succ (+\ (pred\ x)\ y)))\]
\[+_r = Y(\lambda+.(\lambda x.\lambda y.\ if0\ y\ x\ succ (+\ x\ (pred\ y))))\]

Due to functions being error-sensitive, the programmer can manually apply errors in different orders and observe the changes in behaviour. ($+_l\ error_1\ error_2$) evaluates $x$ first and so it returns $error_1$, ($+_r\ error1\ error2$) evaluates $y$ first and so it returns $error_2$. This behaviour is precisely what makes SPCF \textit{observably} sequential and what ultimately enables the construction of a fully abstract sequential language. 

\subsubsection{Observing evaluation order with catch}
The catch operator is introduced as part of SPCF so that a function's order of evaluation may be determined internally as part of a program. The inclusion of errors allows a programmer to try different combinations of errors as function arguments and remember their result, and with some kind of internal construct, catch, there is now a way for programs themselves to determine this.

\cite{cartwright_1992} introduce a family of catch procedures based on the catch construct found in the original version of Scheme. Catch as defined here is a family of procedures with types ($\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow o$, which is to say that if $f$ is a function with type $\tau_1 \rightarrow \dots \rightarrow \tau_n$, $catch\ f$ will return a base type. $catch\ f$ returns the index of the argument in which it is \emph{strict}, which means the argument which is evaluated first. If $f$ evaluates no arguments and returns a constant, then the plus the number of arguments is returned. This catch procedure is equivelantly expressive to the downward catch defined in Scheme but this is slightly simpler to reason about. 

In Chapter \ref{chapter: denesting}, we will discuss in further detail the implications of including errors and catch in SPCF but for now it is worth knowing that control operators will play a fundamental role in the removal of function nesting, where nested calls are effectively replaced by a series of jumps.

\section{Nesting Removal in SPCF}\label{sec:nesting_removal}
It is incredibly common when writing programs to nest function calls. It leads to terse and expressive code but can lead to complicated implementations. A more formal definition of a nested function call can be defined as the sharing of variables from a function's scope as arguments of another function. It is possible to refactor a program, that is to change its representation to an equivalent form without changing its behaviour, to eliminate this behaviour. \cite{laird_2007} outlines a method for this in SPCF by transforming programs to a sublanguage ASPCF and subsequently projecting it back to the original type. Further, he shows that for every term of SPCF $M$ there exists an ASPCF term $M'$ such that $M \simeq M'$.

ASPCF is SPCF with some additional typing constraints, referred to as affine typing. Affine typing restricts the use of variables and removes recursion, enforcing a form of linearity in programs. Commonly, affine typing implies variables may only be used once, but in this case it means variables may have a single reference or copy in use at any given time. Consequently, terms of an application may not share free variables, preventing functions from interfering with one another's evaluation. For example, for an affinely typed term $(A\ B)\ C$, $B$ and $C$ can contain no common free variables. It is for this reason that fixed point recursion is also removed, clearly the inner applications of the Y combinator \eqref{eq:y_combinator} cannot generally guarantee that terms contain no common free variables.

\cite{laird_2007} first considers a bounded version of SPCF where types are finite, for example booleans or a bounded subset of the natural numbers. As well as this, terms are not recursive. What follows are denotational semantics creating a pair of injection and projection relations between SPCF terms and observationally equivalent ASPCF terms. Secondly, an unbounded call-by-value SPCF is considered. Here, numerals are defined over the natural numbers and terms may have recursive definitions. To not violate affine typing rules, recursion is replaced with iteration. From this, it is shown that unbounded terms have an affinely typed observationally equivalent term, and hence the same method is also applicable to unbounded terms.

While it may be possible to refactor unbounded terms, in the real world, it makes little sense to implement and run this process naively for non terminating programs. The refactoring process would also be unbounded, both in execution time and memory usage. One potential usecase is to refactor terms in parts, considering that some paths in an unbounded program may still be bounded and it would be possible to remove nesting for these parts.

The transformation is wholesale, meaning entire branches of the program would have to be considered at once. Importantly it's not possible to denest a function in isolation without also denesting its dependants. The process would have to be done to entire branches or paths of a program at once. A program path is a common conceptualisation of decisions being made in the program, for example conditionals and function calls both fork the program into multiple paths. From this, it is theoretically possible to lazily refactor paths of a program and over time return these to the user. This approach would of course also work for the bounded case, where it would terminate as before rather than having one or many long-standing or potentially infinite tasks. 

\section{Implementation}\label{section: lit-review-impl}
An appropriate representation for SPCF programs in computer memory is needed. This will most likely be a form of abstract syntax tree (AST), which are commonly used in language compilers and parsers. They represent programs as a hierarchical tree of statements which follow the same syntactic ordering of the original program. This provides a convenient and efficient representation to operate on. The AST could be constructed either through tokenising an input string or by hard coding it. For this project we provide methods of doing both.

A very natural way to model an AST is through the use of an algebraic data type (ADT), which is supported well by Haskell. An ADT provides an expressive way to define and combine product types, such as structs and tuples, as well as sum types, such as enums. Their expressiveness and ability to model recursive types make them a natural fit to model inductive definitions such as the grammar for SPCF and recursive data structures like ASTs. \cite{jones_2003} discusses idioms related to abstract syntax tree implementation and agrees that functional languages provide a natural basis for them because of their support for user-defined recursive datatypes. For this interpreter, variables, abstractions, and applications will each have a corresponding representation in the ADT. Constants such as $succ_{o \rightarrow o}$ will not be a part of the ADT but rather functions to construct their relevant terms.

Once an AST is constructed, it will be possible to evaluate terms by implementing the small-step operational semantics set out by \cite{laird_2007}. Each step will be a function which pattern matches on the term's ADT, applying the relevant reduction rule.

In regards to implementing the denesting action, \cite{laird_2007} denotationally outlines the injective and projective mappings between SPCF and ASPCF terms which should be straightforward enough to implement once there is a working representation of SPCF in computer memory. 

\subsection{Testing and Evaluation}
To test the interpreter works, nested and denested terms will be evaluated on arbitrary data and the results will be compared. While term evaluation is still under development, it will be possible to do the evaluations by hand to get the same effect. For bounded terms, it could be possible to map every input and output for the SPCF term and check that they are equivalent to that of the ASPCF term. This should hold since they are observationally equivalent. 

Unit tests could be used for the lower level language concepts such as reduction and term construction.

It could be possible to recreate the proofs set out by \cite{laird_2007} but that may incur a lot of additional work, so we will most likely stick to unit tests and comparison to by hand evaluation to provide an approximation of correctness commonly accepted in software development.

\section{Conclusion}
Laird provides a valuable framework for function nesting removal in SPCF. By tracing the rich historical lineage of SPCF from PCF, LCF, and the lambda calculus, we contextualise function denesting within a broader scope. We discuss some potential applications of unbounded function denesting, as well as ideas for implementation. Haskell is a fitting language for implementation due to its support of algebraic datatypes and lazy evaluation. With a well defined basis, the project's implementation is a natural next step.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{SPCF Interpreter}\label{chapter: interpreter}
This chapter chapter aims to discuss a majority of the required background material for the next Chapter, which discusses the denesting action in detail. This action relies on a wide range of functional programming concepts, here we hope to build on Section \ref{section: lit-review-impl} along with notes on their implementation.

\section{Execution model}\label{section: language representation}
An adequate runtime execution model is required if we are to construct and evaluate terms in SPCF. Haskell is a popular choice for building interpreters for variants of the $\lambda$-calculus and a common approach to representing terms are as an algebraic data type (ADT). This is illustrated in Listing \ref{listing:spcf-ast}, each term of the grammar \eqref{eq:spcf_grammar} is represented as a constructor. In terms of abstract syntax trees, leaf nodes represent variables and constants whereas branches are applications, abstractions, and functional constants. 

Many aspects of SPCF are shared by other variants of lambda calculus, such as variables, abstractions, applications, and common function constants such as the successor and predecessor. What is in some senses interesting about SPCF is both the typeable fixed point combinator borrowed from LCF and its inclusion of a non-local operator 'catch'. 

\begin{listing}
\caption{SPCF AST inductive definition using an ADT}
\begin{minted}{haskell}
data Term
  = Numeral Int
  | Error Error
  | Variable Label
  | Lambda Label Type Term
  | Apply Term Term 
  | Succ Term
  | Pred Term
  | YComb Term
  | If0 Term Term Term
  | Catch Term
\end{minted}
\label{listing:spcf-ast}
\end{listing}

\subsection{Capture avoiding substitution}
Similarly to any other variant of the $\lambda$-calculus, SPCF assumes well defined definitions for $\alpha, \beta, \eta$ equivalence, and of course, capture avoiding substitution. Our implementation includes these, with the notable omission of $\eta$-expansion because it is not relied upon by other operations. Capture avoiding substitution is often glossed over when discussing abstract language definitions, however seeing as our implementation is concerned with a concrete language definition, we shall briefly discuss it.

Central to capture avoiding substitution is the notion of a fresh variable. Fresh variables are valid variable names which are not already bound in a term. We use Haskell's lazy evaluation to construct an infinite list of variable names of the following form: 
\[
\begin{split}
&vars := \{"a", \dots, "z"\} \cup \{\ ci\ |\ c \in \{"a", \dots, "z"\}, i \in \mathbb{N}\} \\
&\text{where $ci$ is the concatenation of $c$ and $i$.}
\end{split}
\]
A fresh variable is then chosen by taking the first element of the set which is not already used in the current term. This method guarantees the existence of a fresh variable and minimises computational overhead as it is lazy. Then, after being sure to rename bound instances of the given variable to the fresh variable, substitution can be carried out as normal.

\subsection{Bounded and Unbounded SPCF}
We introduce a bounded variant of SPCF which will serve as a basis for the denesting action working on bounded terms later. This is achieved primarily by removing fixed point recursion and limiting numerals to be smaller than an upper bound $n$, but some other simplifications have been made such as removing if0, succ, pred, and error constants. Additionally, it is extended with binary products, $n$-fold products, and the conditional operator 'case'.  

\begin{equation} \label{eq:bounded_spcf_grammar}
\begin{split}
    M,N &:= B\ |\ F\ |\ x^{\tau}\ |\ (\lambda x^{\sigma} .M^{\tau})^{\sigma \rightarrow \tau}\ |\ (M^{\sigma \rightarrow \tau} \ N^{\sigma})^{\tau}\ | \ M \times N\\
    B &= \{\underline{n}\ |\ n\in \mathbb{N}\}\\
    F &= \text{catch}_{m}^{\ \tau_1 \rightarrow \dots \tau_n \rightarrow o}\ |\ \text{case}^{o \rightarrow o^n \rightarrow o}\\
\end{split}
\end{equation}

\begin{equation}
    \tau , \sigma := o\ |\ \sigma \rightarrow \tau\ |\ \sigma \times \tau\ |\ \underline{0}
\end{equation}

\textbf{Products}

This definition of relies on a suitable encoding for binary as well as $n$-fold products along with projections $\pi_i : \tau^n \Rightarrow \tau$ and $\pi_1 : \tau \times \sigma \Rightarrow \tau$, $\pi_2 : \tau \times \sigma \Rightarrow \sigma$. $n$-fold products provide a convenient syntax for constructing long collections of terms of uniform type. We provide an implementation of n-fold products by abstracting over a list and making use of its built in functions for traversal and indexing. The empty product $I$ is simply the empty list.

\section{Types}\label{section: typing}
Types encode additional information about terms, including how they can be applied to one another. Similarly to other typed lambda calculi, types in SPCF can be encoded as either a 'base' type $o$ or an 'arrow type' $\sigma \rightarrow \tau$. This set has been expanded to include an explicit 'Cross' type to encode n-fold and binary products as well as an 'Empty' type to typify non-terminating programs, which is necessary for programs written in continuation passing style, which the denesting action relies upon.

\begin{listing}
\caption{SPCF inductive definition for types using an ADT}
\begin{minted}{haskell}
data Type
  = Base            -- Base type (numerals and errors)
  | Empty           -- Return type of a non-terminating function
  | (:->) Type Type -- Function
  | Cross Type Type -- Product
\end{minted}
\label{listing:type-adt}
\end{listing}

\begin{definition}
    A typing \emph{context} $\Gamma$ is a map of variables to types.
\end{definition}

A typing context can be represented as a list, for example, $\Gamma = a:\tau, b:\sigma, \ldots$. It is also common to extend contexts, for example, $\Gamma, x:\tau$ reads as the context $\Gamma$ extended with $x$ having type $\tau$. 

\begin{definition}
    A typing \emph{judgement} $\Gamma \vdash M:\tau$ is a proposition, stating that given a context $\Gamma$, the term $M$ has type $\tau$. A judgement can be true or false, depending on the given context.
\end{definition}

\begin{definition}
    A typing \emph{rule} determines if it is possible for a term to have a type, sometimes this is called being well typed. Rules are written like this
    \[
    \frac{Premise_1 \quad\quad Premise_2 \quad\quad \ldots \quad\quad Premise_n}{Conclusion}
    \]

    This can be read 'given each of the things on the top, the thing on the bottom must be true'. It is possible to have a rule with nothing on the top, which is similar to an axiom or a statement which is always true.
\end{definition}

\subsubsection{Typing rules} \label{section: type rules}

\textbf{Variables}
\begin{equation}
    \frac{}{\Gamma, x:\tau \vdash x: \tau}
\end{equation}

\textbf{Abstraction}
\begin{equation}
    \frac{\Gamma, x:\sigma \vdash M : \tau}{\Gamma \vdash \lambda x^{\sigma}. M : \sigma \rightarrow \tau}
\end{equation}

\textbf{Application}
\begin{equation}
    \frac{\Gamma \vdash M : \sigma \rightarrow \tau \quad\quad \Gamma \vdash N : \sigma}{\Gamma \vdash MN : \tau}
\end{equation}

\textbf{Product}
\begin{equation}
    \frac{\Gamma \vdash M:\sigma \quad\quad \Gamma \vdash N:\tau}{\Gamma \vdash \langle M,N \rangle : \sigma \times \tau}
\end{equation}

\textbf{Empty Product}
\begin{equation}
    \frac{}{\Gamma \vdash \langle \rangle : I}
\end{equation}

\textbf{Projection}
\begin{equation}
    \frac{\Gamma \vdash M:\tau_1 \times \tau_2}{\Gamma \vdash \pi_i M: \tau_i} i \in \{1, 2\}
\end{equation}

\subsection{Implementation details} 
There are two primary approaches to interpreting typing rules depending on the order in which layers are read. If read top to bottom, rules describe a type verification algorithm which ensures terms adhere to type rules. If read bottom to top, they describe a type inference algorithm capable of determining the type of a given term. Both methods traverse a term's syntax tree applying type rules at each sub-expression and both rely on the fact that every term in SCPF has a unique type.

For the purposes of this project, a type inference algorithm is necessary to construct a general application of the denesting action described in Chapter \ref{chapter: denesting}. The action relies on the type structure of terms it is applied to and in some senses actually describes a family of actions, each of which acting on terms of a specific type. The tree traversal and book keeping of variables is done in much the same way as for term evaluation, described in Section \ref{section: evaluation}, with the notable addition of applying the type rules specified in \ref{section: type rules}.

One might wonder why the implementation of the denesting action, which ultimately takes the form of a pair of terms in the language, is written in Haskell and not in SPCF. That is to say, if our implementation is fully expressive why must we rely on Haskell to construct these terms? The answer lies once again as a typing problem. We have described the typing system of SPCF as monomorphic, where each term has a unique type and the type inference algorithm to be similar to the original definition by \cite{curry1958combinatory}. To define a term across many types as would be needed here we would required a type system supporting parametric polymorphism. Without this, it is possible to exhaustively define pairs of terms for a finite subset of types, but nothing more. This would be possible and indeed type inference algorithms for these type systems exist \cite[\emph{e.g.},][]{hindley_1969, milner_1978} however they are more complex. Therefore we utilise on Haskell's more advanced type system to define these terms generically without the additional implementation complexity. 

\subsection{Examples}
Consider $+_l$ defined in Section \ref{sec:spcf}, its typing judgement would look like \ref{listing: addLeft judgement}. The program output for the typing of this term is illustrated in Listing \ref{listing: addLeft judgement}. More typing terms and their typing judgements can be found in Appendix \ref{appendix: term evaluations}.
\begin{listing}
\caption{Program output from the typing of $+_l$}
\label{listing: addLeft judgement}
\begin{verbatim}
Type judgement for addLeftTerm = \f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))
[f]: o->o->o
[x]: o
[y]: o
[pred x]: o
[f (pred x)]: o->o
[f (pred x) y]: o
[succ (f (pred x) y)]: o
[if0 x then y else (succ (f (pred x) y))]: o
[\y:o => if0 x then y else (succ (f (pred x) y))]: o->o
[\x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: (o->o->o)->o->o->o

Type judgement for add = \x:o => \y:o => (fix addLeftTerm) x y
[x]: o
[y]: o
[fix addLeftTerm]: o->o->o
[(fix addLeftTerm) x]: o->o
[(fix addLeftTerm) x y]: o
[\y:o => (fix addLeftTerm) x y]: o->o
[\x:o => \y:o => (fix addLeftTerm) x y]: o->o->o
\end{verbatim}
\end{listing}

\section{Term Evaluation}\label{section: evaluation}
Typically when reducing expressions it is tempting to only consider closed terms, terms with no free variables. However, to define rigorous operational semantics, we must consider all scenarios. It is common to represent the rules for reducing expressions as a series of 'small-step' operational semantics, which provide rules on what to do for a given term. Each step could be applied at any point in the computation and as such there could be free variables to account for. A common way of defining these rules is through the use of evaluation contexts.

It is worth noting that not all calculi require a notion of evaluation contexts to define evaluation. In a more classical example of a simply typed lambda calculus, basic datatypes are cleverly represented through a series of higher-order functions, called Church encodings. In that example, a term's computation can be defined as its $\beta$-reduction. It is our more complex language definition which calls for the need for evaluation contexts when defining its evaluation rules.

\begin{definition}
    A \emph{closure} $(E, t)$ is an environment $E$ paired with a term $t$ such that the environment is defined for all free variables in $t$.
\end{definition}

\begin{definition}\label{def: environment}
    An \emph{environment} $E$ maps labels to closures. $E$ \emph{interprets} a label $x$ if there exists a map from $x$. The result to which $E$ interprets $x$ is written as $E[x]$.
\end{definition}

\begin{definition}
    A closure \emph{evaluates} $(E, t) \Downarrow v$ to value $v$ if there exists a value $v$ such that $M \rightarrow^* v$.
\end{definition}

Evaluation contexts are, in essence, an environment and a 'hole' $E[\_]$ which can be filled by any term. When a hole is filled, all the terms free variables take on values from the environment. This naturally describes a way of providing inputs or starting values to a program, as well as equally describing the tracking of an ongoing computation through recording the current values of variables in the environment. This idea of capturing the current state of a computation in a higher kinded type is common in functional programming and is an idiomatic use case for a monad.

A monad represents computations which can be composed together to form new computations. In practice, they are commonly used to track state or to encode the possibility of computational failure. Although monads are a useful abstraction over composable computations, frustratingly they themselves are not composable with one another. To encode multiple monadic effects for a single computation, one must build what is known as a monad transformer, effectively a monad over other monads.

Our implementation of term evaluation in Haskell revolves around a monad transformer we have defined called Eval. It is composed of predefined monads from the Haskell standard library. As illustrated in Listing \ref{listing: eval} it uses a 'Reader' monad to keep track of the current environment, an 'Except' monad to indicate that computation could fail as a string, and a 'Writer' monad to keep track of any supplementary logs which are a useful way of recording the order of operations to look at later. An environment is just a map of labels to terms, exactly as it is defined in Definition \ref{def: environment}. Using this monad, it is possible to define an 'eval' function with the signature below:
\mint{haskell}|eval :: Term -> Eval Term|
This reads as 'eval is a function which maps a term to the Eval of that term'. 

\begin{listing}
\caption{The Eval monad used for term evaluation}
\begin{minted}{haskell}
type Environment = Map.Map Label Term
type Eval a = (ReaderT Environment (ExceptT String (WriterT [String] Identity))) a
\end{minted}
\label{listing: eval}
\end{listing}

Recall that Term (Listing \ref{listing:spcf-ast}) is an algebraic data type, meaning that eval must provide a case for each possibility of what that term could be. This is called exhaustive pattern matching and is commonly used alongside monad transformers to build interpreters, \citep[\emph{e.g.},][]{Liang1995MonadTA}. Each matched pattern corresponds to an evaluation rule of the language, these can be found in Section \ref{section: op sem}.

\subsection{Operational semantics}\label{section: op sem}
Evaluation rules are defined similarly to typing rules and together form a small-step operational semantics.

\textbf{Constants}
\begin{equation}
    \frac{}{E[n\in\mathbb{N}] \Downarrow n}
\end{equation}

\begin{equation}
    \frac{}{E[error_i] \Downarrow error_i}\quad(i \in \{1,2\})
\end{equation}

\textbf{Variables}
\begin{equation}
    \frac{E[x] \Downarrow (E', x') \quad\quad E'[x'] \Downarrow V}{E[x] \Downarrow V}
\end{equation}

\textbf{Abstraction}
\begin{equation}
    \frac{}{E[\lambda x. M] \Downarrow E[\lambda x. M]}
\end{equation}

\textbf{Application}
\begin{equation}
    \frac{E[M] \Downarrow (E', \lambda x. x') \quad\quad  E[N] \Downarrow V' \quad\quad E'\cup \{x\mapsto V'\}[x'] \Downarrow V}{E[MN] \Downarrow V}
\end{equation}

\textbf{Successor}
\begin{equation}
    \frac{E[M] \Downarrow n}{E[succ\ M] \Downarrow n + 1}
\end{equation}

\textbf{Predecessor}
\begin{equation}
    \frac{E[M] \Downarrow n + 1}{E[pred\ M] \Downarrow n}
\end{equation}

\textbf{If0}
\begin{equation}
    \frac{E[P] \Downarrow 0 \quad\quad E[M] \Downarrow V}{E[\text{if0}\ P\ \text{then}\ M\ \text{else}\ N] \Downarrow V}
\end{equation}

\begin{equation}
    \frac{E[P] \Downarrow n \quad\quad n > 0 \quad\quad E[N] \Downarrow V}{E[\text{if0}\ P\ \text{then}\ M\ \text{else}\ N] \Downarrow V}
\end{equation}

\textbf{Y combinator}
\begin{equation}
    \frac{}{E[\text{Y} \lambda x. M] \Downarrow E[M[\text{Y} \lambda x. M / x]]}
\end{equation}


\textbf{Catch}
\begin{equation}
    E[\text{catch}\ M] \Downarrow i \text{ Where i is where M is strict. See Section \ref{section: catch}.}
\end{equation}

\subsubsection{Bounded term evaluation}
For the bounded version of SPCF there are additional evaluation rules needed for products and the conditional statement case. 

\subsection{Observing sequentiality with Catch}\label{section: catch}
One very important and helpful insight into the behaviour of a program is the order in which it evaluates its arguments. It may not be immediately obvious but armed with that knowledge it is possible to completely define a procedure's behaviour. One way for programmers to determine a procedure's evaluation order is by using errors, by applying an error in different positions it is possible to observe which position leads to an error.

SPCF forces all procedures to be error-sensitive, meaning that if an error is evaluated it is immediately returned. For example, $+_l$ $error1$ $error2$, using $+_l$ defined in Section \ref{section: observing sequentiality}, would return $error1$. The inclusion of catch into SPCF allows programs to observe evaluation orders. \cite{cartwright_1992} define it as a family of procedures with types $(\tau_1 \rightarrow \dots \tau_n \rightarrow o) \rightarrow o$ where if $f$ is a procedure of type $\tau_1 \rightarrow \dots \tau_n \rightarrow o$, then catch $f$ returns $i - 1$ if f is strict in its $i$th argument and $k+n$ if $f$ returns a constant. 

In our implementation of Catch, the AST continues to be walked, but in a slightly different way. Like before function constants and applications essentially continue the downward traversal of catch, being careful to evaluate applications and Ifzero in sequential order of their arguments. Things become more interesting when evaluating a lambda abstraction. The label of the abstraction is pushed to a stack maintained by the catch function before continuing downwards. When a variable is reached, the index of the last occurring instance of the label in the stack is returned. If an error is reached, that error is returned, and if a numeral constant is reached,  that value along with the length of the current stack is returned. Catch was originally defined in terms of a lexically scoped control stack, which equally describes this implementation.

Catch encodes non-local control into SPCF programs, meaning it is possible to stop a term's execution prematurely and jump to somewhere else in the program. This feature is not normally present in $\lambda$-calculi and is one of the things which uniquely identifies SPCF from its peers. We discuss further the implications of the existence of catch in Chapter \ref{chapter: denesting}, along with how it is used to construct the affinely definable denesting transformation.

\subsection{Examples}

\section{Parser}
As part of the project, we have provided a lexer and parser for SPCF with the goal to make experimentation with the language easier and to bring it closer to practical programming languages. A lexer and parser makeup what is known as a compiler frontend and together they enable writing programs as plain text with some syntax sugar, making programs significantly easier to read and write. The execution pipeline looks like this:
\[\text{Lexical analysis -> Parse tokens -> Object code -> Execute object code}.\]

\subsection{Lexical analysis}
Lexical analysis, also called tokenisation, is where source code is decomposed into a stream of tokens. Using an incredibly simple example, the code `\lstinline{x = 5;}` could be decomposed into the tokens ["x", "=", "5", ";"]. We are effectively classifying substrings according to what they do. Patterns for these tokens are defined ahead of time along with the language definition. It is this list of tokens which is used by the parser to construct an AST. The action of \emph{lexing} is conceptually similar to applying a series of nested regular expressions.

Tokenisation is a well-defined problem and general tools exist to the make the job easier. Alex\footnote{https://haskell-alex.readthedocs.io/en/latest/about.html} is a code generation tool used to build lexers in Haskell. It is configured with a set of regular expressions, which can be constant strings, which have a one-to-one correspondence with a token. Tokens are defined as entries in an algebraic datatype which can be referenced from other parts of the compilation pipeline. Once configured, Alex will generate Haskell code for a lexer defined on those expressions and bundle it with the interpreter binary. It works similarly to a large macro, abstracting many mundane details 
which would otherwise have to be considered if handwriting a lexer, while highlighting the important ones such as which tokens exist.

One benefit of Alex being built with Haskell is that it has extraordinary support for functional idioms, such as using a State Monad for tracking token information during tokenisation. This monad keeps track of metadata such as the file name and current character position, which is used to make errors more informative. 

Using the previous example along with the minimal Alex configuration in Listing \ref{listing: alex config}, it is possible to tokenise the expression \lstinline{x=5;} as the list of tokens \lstinline{[(TokenId "x"), TokenEquals, (TokenNat 5), TokenSemicolon]}. If the expression was malformed such as \lstinline{x@ = 5;}, then the error will include information from the state monad
\begin{verbatim}
spcf: user error (programs/program.spcf:1:2: lexical error at character '@')
\end{verbatim}

\begin{listing}
\label{listing: alex config}
\caption{Minimal Alex configuration to lex assignment operations.}
\begin{verbatim}
-- The top half configures regular expressions to be used when matching tokens
$digit = 0-9
$alpha = [A-Za-z]
tokens :-
  $digit+                       { lex (TokenNat . read) }
  \=                            { lex' TokenEquals }
  \;                            { lex' TokenSemicolon }
  $alpha [$alpha $digit \_ \']* { lex (TokenId) }
{
\end{verbatim}
\begin{minted}{haskell}
-- The bottom half configures the ADT tokens will inhabit
-- A token is a pair consisting of the Alex State monad and the TokeClass ADT.
-- This is to improve error messages.
data TokenClass = TokenId String | TokenSemicolon | TokenNat Int | TokenEquals
data Token = Token AlexPosn TokenClass
}
\end{minted}
\end{listing}

\subsection{Parsing} 
A parser uses a set of recursive rules to construct the program AST from the stream of tokens generated during lexical analysis. Similarly to tokenisation, parsing is a well-defined problem and tools exist to abstract laborious details. Happy\footnote{https://haskell-happy.readthedocs.io/en/latest/} is a code generation tool used to build parsers in Haskell, it is based on the famous parser-generator Yacc, which notably has a syntax reminiscent of BNF grammars. Similarly to Alex, Happy configuration can be viewed as a macro, only slightly more complex. It can be read top-down as a series of rules, where each rule may be defined inductively in terms of other rules. 

Using the same example from before and the minimal Happy configuration in Listing \ref{listing: happy config} the expression \lstinline{x=5;} could become encoded in the intermediate AST representation as: 
\begin{minted}{haskell}
Declare "x" (Numeral 5)
\end{minted}

Similarly to Alex, a State monad is used to capture information from parsing to improve error messages. Notice that the pattern matching cases in Listing \ref{listing: happy config} are not exhaustive, if no suitable rule is matched then a parse error is thrown and the programmer is alerted to where the error is. For example the program \lstinline{x = ;} throws the error
\begin{verbatim}
spcf: user error (programs/program.spcf:1:5: parse error at token ';')
\end{verbatim}

As a testament to how powerful Happy and Alex are when used together, both the lexer and parser for the Glasgow Haskell Compiler (GHC) are entirely defined using them\footnote{https://github.com/ghc/ghc/blob/271a7812cbb47494c74b1dc3b7d2a26fd8d88365/compiler/GHC/Parser/Lexer.x}. 

\begin{listing}
\caption{Minimal Happy configuration to parse assignment expressions}
\label{listing: happy config}
\begin{minted}{haskell}
-- Atomic terms have a one to one correspondence with a term in the AST.
-- Map TokenId to AST.Variable
-- Map TokenNat to AST.Numeral 
ATerm :
  | natVal { case $1 of Token info (TokenNat n) -> SPCF.AST.Numeral info n }
  | id { case $1 of Token info (TokenId id) -> SPCF.AST.Variable info id }

-- In this case, statements may only bind values to names.
-- Map TokenId to a declaration using what is defined in `Binder`.
Statement : id Binder { 
    case $1 of Token info (TokenId id) -> SPCF.Interpreter.Declare info id $2 }

-- Determine what can be on the RHS of declarations
Binder : '=' ATerm { $2 }
\end{minted}
\end{listing}

\begin{listing}
\caption{Addition defined in SPCF}
\begin{minted}{haskell}
addLeftTerm = \f:Nat->Nat->Nat => 
              \x:Nat => 
              \y:Nat => if x then y else (succ (f (pred x) y));
add = \x:Nat => \y:Nat => (fix addLeftTerm) x y;
eval (add error1 error2);
\end{minted}
\end{listing}

\section{Correctness Testing}


\section{Conclusion}
The goal of implementing the SPCF denesting action defined by Laird relies on a large amount of background material and prerequisite tools working together. We supply a suitable runtime representation for SPCF and method for evaluating terms. We expand on this, using code generation tools to provide an interpreter frontend for the language, allowing for easier experimentation and bringing it closer to a practical programming language. We discuss a suitable approach to typing and how the denesting action requires us prefer type inference over type checking. Special attention is given to the catch term as that is one of uncommon features of SPCF. At each stage, definitions and examples are used to reinforce points where necessary.

With everything defined in this chapter, it is possible to next discuss in more detail the denesting action and implementation implications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Affine transformation of programs}\label{chapter: denesting}
Up to this point, we have discussed SPCF and its predecessors \ref{sec:spcf}, the concrete language definitions used \ref{section: language representation}, how they were implemented in the form of an interpreter \ref{section: evaluation}, and alluded to a refactoring transformation for programs written in SPCF to an equivalent affine form \ref{sec:nesting_removal}. We build on these and elaborate on the refactoring action, how it uses SPCF-specific features, and comment on implementation details. Our understanding is that this is the first time this transformation has been implemented in the real world. 

Recall that the goal of this transformation is to construct observationally equivalent terms which satisfy affine typing rules. 

\section{Procedures as decision trees}
Graphs are often used to encapsulate certain aspects of a program or function's behaviour. These can be at any level of abstraction, such as a high level visualisation of the neurons in neural networks, or the low level control flow of a function's possible execution paths at runtime. The choice to include non-local control and error-sensitivity in SPCF means that its graph model for functions will have more structure than generic function graphs. Intuitively, it makes sense that the graph model for a sequential language should have to encode some essence of sequentiality, specifically the evaluation order of function arguments. This is exactly the case, one of the key findings of \cite{cartwright_1992}, was a denotational model for continuous functions in SPCF constructed from decision trees. 

Consider the term:
\[\lambda xyz.\ \text{if0}\ x\ \text{then}\ (succ\ y)\ \text{else}\ (pred\ z)\]
It is strict in its first argument, the strictness of its second and third arguments are determined by the value of x. If x is 0 then it will evaluate y and z otherwise. Thus this function is entirely representable by the index at which it is strict, along with a mapping of all possible input values to their respective outputs; this is written $\langle i, f \rangle$. Since functions are error sensitive, error1 and error2 will always map to error1 and error2 respectively, and since it is continuous $\bot$ always maps to $\bot$. Thus the function above can be represented as

\[\langle 1, 
\begin{cases}
    \bot \mapsto \bot\\
    \text{error}_1 \mapsto \text{error}_1\\
    \text{error}_2 \mapsto \text{error}_2\\
    0 \mapsto\ \langle 2 \begin{cases}
        \bot &\mapsto \bot\\
        \text{error}_1 &\mapsto \text{error}_1\\
        \text{error}_2 &\mapsto \text{error}_2\\
        0 &\mapsto 1\\
        1 &\mapsto 2\\
          &\dots
    \end{cases} &\rangle\\
    1 \mapsto\ \langle 3 \begin{cases}
        \bot &\mapsto \bot\\
        \text{error}_1 &\mapsto \text{error}_1\\
        \text{error}_2 &\mapsto \text{error}_2\\
        0 &\mapsto \bot\\
        1 &\mapsto 0\\
        2 &\mapsto 1\\
          &\dots
    \end{cases} &\rangle\\
    \dots
\end{cases} \quad\quad\rangle
\]
This can be represented as a decision tree with vertices being the set of pairings  $\langle i, f \rangle$ and edges leading from each vertex being the set of possible values of the $i$th argument. 

Other models for SPCF have been worked on, for example \cite{kanneganti_1993} provide an alternate model, reconstructed from this one, using the idea of a prime basis \citep{Winskel1980EventsIC} in an attempt to show that SPCF is a simple extension of the simply typed lambda calculus. We will focus on this original decision tree model because the idea of representing functions as trees is a helpful mental model to have when reasoning about the denesting action discussed in the next section.

\section{Procedures as tuples}
\begin{definition}
    A \emph{continuation} is a higher-order function representing the current state of a computation.
    
    Classically, continuations have one argument, the value of the computation so far, and return the final result of the computation when the rest of the program has also finished. They are analogous to callback functions or partial function applications in practical programming languages. 
\end{definition}

The 'continuation passing style' (CPS) of programming is a style of programming where all function calls are tail calls. It is possible to have an entry point to the program have a return type being the empty type $0$ and all computation be through the calling of continuations. In some senses, CPS programming is the natural opposite of classical functional programming. With the latter, all computations are effectively the return values of functions, whereas with computations are instead function side effects captured by running continuations.

\cite{laird_2007} relies on the notion of continuation passing to define his inj function which effectively represents bounded terms as a series of nested tuples consisting of the index at which a function is strict and a tuple of continuation functions for all the possible values at its strict argument. 

The basis of the transformation of programs into an equivalent affinely typeable form set out by \cite{laird_2007} is a pair of injection and projection functions. The injection maps a term to a tuple of the index at which it is strict and a collection of continuation functions for each possible value which could have been applied.  

\begin{definition}
    The insertion of a value $x$ at the position $i$ into a given tuple $t = \langle t_0, \dots, t_{n-1} \rangle$ where $i \leq n$ is denoted $t\lfloor x \rfloor_i$. For example $t\lfloor x \rfloor_i = \langle t_0, \dots, t_{i-1}, x, t_i, \dots, t_{n-1} \rangle$
\end{definition}

\begin{definition}
    The removal of a value $x$ at the position $i$ from a given tuple $t = \langle t_0, \dots, t_{n-1} \rangle$ where $i \le n$ is denoted $\lceil t \rceil_i$. For example $\lceil t \rceil_i = \langle t_0, \dots, t_{i-1}, t_{i+1}, \dots, t_{n-1} \rangle$
\end{definition}

\subsubsection{Injection to tuple form}
\begin{definition}
    The function \emph{inj}, which transforms a term in the bounded SPCF to a so-called 'tuple form', is definable as \\
    inj: $\llbracket \underline{n}^{m+1} \Rightarrow \underline{0} \rrbracket \rightarrow \llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \rrbracket$\\\\
    inf($f$) =
    $\begin{cases}
        f &\text{if}\ f \in \{\top, \bot\}\\
        \langle i, \langle f'_j\ |\ j < n \rangle \rangle\ &\text{if}\ i = \text{catch } f  \text{, where } f'_i(x) =  f(x \lfloor j \rfloor_i)
    \end{cases}$\\\\
   This is definable as the term
    \[
    \lambda f.\text{case}\langle \text{catch}\ f, \langle \langle j, \langle \pi_0(x),\dots,\pi_{m-1}\rangle \lfloor j \rfloor_i\ |\ i \leq m \rangle \rangle\ |\ j \le n \rangle
    \]
\end{definition}

To understand this term it helps to consider its type one component at a time and correspond that to its operational definition. $f$ is the only parameter of proj and has the type $\underline{n}^{m+1} \Rightarrow \underline{0}$, which means that $f$ is a function mapping $\underline{n}^{m+1}$, an $m+1$ fold product of numerals $\le$ n, to the empty type $\underline{0}$. A valid question would be to ask what types of this term look like and the answer would be uncurried functions written in a continuation passing style. The second half of inj's type is a pair with the first element being $\underline{m+1}$, precisely the type provided by 'catch $f$' and a tuple containing the same number of candidate continuation functions as there are numerals in the language. Each continuation has the type $\underline{n}^{m} \Rightarrow \underline{0}$ which can be seen as the same type as $f$, where it takes one less argument. When regarding the operational definition it can be seen that this is because an argument has already been applied in the strict position as part of the continuation.

It must also map $\top$ to $\top$ and $\bot$ to $\bot$ to remain \emph{continuous}, a property that functions must have in the underlying domain SPCF is defined on.

\subsubsection{Projection from tuple form}
\begin{definition}
    The function \emph{proj}, which transforms a term in tuple form back to its original type, is definable as \\
    proj: $\llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \rrbracket \rightarrow \llbracket \underline{n}^{m+1} \Rightarrow \underline{0} \rrbracket$\\\\
    proj($\langle i, c\rangle$)($x$) =
    $\begin{cases}
        i\ &\text{if}\ i \in \{\top, \bot\}\\
        \top\ &\text{if}\ \pi_i(x) = \top, \text{ or } \pi_i(x) = \top\\
        \top\ &\text{if}\ f'_i(\lceil x \rceil_i) = \top \text{, where } f'_i = \pi_j(c)\\
        \bot\ &\text{otherwise}
    \end{cases}$\\\\
    This is definable as the term
    \[
    \lambda tx. \text{case } \langle \pi_1(t), \langle \text{case } \langle \pi_i(x), \langle \pi_j(\pi_2(t)) \lceil \langle \pi_0(x), \dots, \pi_m(x)\rangle\rceil_j\ | \ j \le n \rangle \rangle\ |\ i \leq m \rangle \rangle
    \]
\end{definition}

To understand this term, it helps to bear in mind the structure of $f$ when in tuple form. The parameter $t$ is of the form $\langle i, c\rangle$ where $i$ is the index at which $f$ is strict and $c$ is a tuple of continuation functions for each possible value supplied at the strict position. The parameter $x$ is the tuple of $f$'s parameters. In that sense the term defined here is a curried function and can be seen as having the type $\llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \times \underline{n}^{m+1} \rrbracket \rightarrow \llbracket \underline{0} \rrbracket$.

\subsection{Implementation details}
The definitions of inj and proj are quite dense and require many working features prior to their implementation. In Chapter \ref{section: language representation} we discuss most of these, however it is worth noting some subtleties which may not be obvious:
\begin{enumerate}
    \item Care is needed when defining if function arguments and tuples are 0-indexed or not. Both terms rely on the definition of catch to index a tuple, so their indexing must be consistent.
    \item Extra care is required when choosing a representation for tuples. There are two cases in the definitions of inj and proj, the n-fold product and the binary product. 
    \item The construction of each term depends on the type of their input, thus a suitable method of type inference is required. This influenced the decision to rely on type inference for type checking discussed in Chapter \ref{section: typing}.
\end{enumerate}

To expand on point (2), recall that the $n$-fold is a product with length $n$ where each element is of the same type, and that the binary product has no such restriction on typing. This should be clear by the fact inj returns a product of two elements with different types, a numeral and another product. It follows that it is not possible to define this pairing as an n-fold product and have the program be correctly typed. One solution is to construct all n-fold products from the repeated application of binary products, however having a representation for n-fold products relying on an underlying list is very convenient and provides many ready built functions such as 'map' to apply the same transformation to all elements. The alternative is to include definitions for both kinds of product including $\pi_1 \text{ and } \pi_2$ terms for the binary case. This enables the type to be correctly and statically determined. 

\subsection{Affinely definable retraction on types}
\begin{definition}
    A \emph{retraction} $X \trianglelefteq Y$ is a function $f: Y \rightarrow X$ where there exists a function $g: X \rightarrow Y$ such that $g \circ f = id_x$. 
    
    A retraction is in some senses a weaker form of bijection, where elements from $X$ can be retrieved from $Y$ after an initial mapping without information loss, but without the implication the function being invertable.
\end{definition}

For types $\tau$ and $\sigma$ in SPCF (and bounded SPCF) the retraction $\tau \trianglelefteq \sigma$ is ASPCF defineable by $\text{inj}: \tau \rightarrow \sigma$ and $\text{proj}: \sigma \rightarrow \tau$. For example $\text{proj}(\text{inj}(f))= f$.

If $\tau_1 \trianglelefteq \tau_2$ and $\sigma_1 \trianglelefteq \sigma_2$, then $\tau_1 \times \sigma_1 \trianglelefteq \tau_2 \times \sigma_2$ and $\tau_1 \Rightarrow \sigma_1 \trianglelefteq \tau_2 \Rightarrow \sigma_2 $.

\section{Transforming unbounded terms}

\section{Conclusion}




Refactoring is the action of rewriting code without changing its behaviour. Many popular forms of refactoring exist 

The transformation is comparable to a form of refactoring. 

how it is done 

that will take a while.

why it works.

Some insights and intuitions from theauthor to demonstrate understanding.

How is it fully expressive

What tie ins are there? categories

Given a working interpreter
and language representation, what are we doing here?

\subsection{Injection}
\subsection{Projection}

how it is different from general SPCF
\section{Unbounded Terms}
how it is different from general SPCF

\chapter{Results}
why is this better?
is it more readable?
some comments here even if brief would create a nice conclusion

How much it grows. Example term with differing upper bound. Then differening type.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}

%%
%% Now we are back to the standard project contents that you should include
%%

This is the chapter in which you review the major achievements in the light of your original objectives, critique the process, critique your own learning and identify possible future work.

highlight novelties.

\vfill
\section{Word Count}
Number of words until this point, excluding front matter: XXX.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%%
%% Use the appendix for major chunks of detailed work, such as these. Tailor
%% these to your own requirements
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Design Diagrams}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{User Documentation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Raw Results Output}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Code}

\section{Lexer}\label{appendix: lexer}
First macros are defined for digits and string characters. Then list of token patterns is defined using those macros. Then an algebraic datatype is used to enumerate all possible tokens, this is primarily what will be used later in the parser. Finally, some helper functions are defined to handle errors during lexing and for the handling of the previously defined ADT.

\lstinputlisting[basicstyle=\scriptsize]{Lexer.x}

\section{Parser}\label{appendix: parser}
The same set of tokens defined in the lexer are brought forward to the definition of our parser. It's this compatibility between happy and alex that make them so popular. Then a series of rules are defined for how to inductively construct an AST from the defined tokens. Finally, some helper functions are defined enabling our program to interface with the parser later on. 

\lstinputlisting[basicstyle=\scriptsize]{Parser.y}

%% NOTE For this to typeset correctly, ensure you use the pdflatex
%%      command in preference to the latex command.  If you do not have
%%      the pdflatex command, you will need to remove the landscape and
%%      multicols tags and just make do with single column listing output

%\begin{landscape}
%\begin{multicols}{2}
%\section{File: yourCodeFile.java}
%\lstinputlisting[basicstyle=\scriptsize]{yourCodeFile.java}
%\end{multicols}
%\end{landscape}

\chapter{Term evaluations}\label{appendix: term evaluations}
Type judgements from the provided example program.
\begin{minted}{haskell}
Type judgement for addLeftTerm = \f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))
[f]: o->o->o
[x]: o
[y]: o
[pred x]: o
[f (pred x)]: o->o
[f (pred x) y]: o
[succ (f (pred x) y)]: o
[if0 x then y else (succ (f (pred x) y))]: o
[\y:o => if0 x then y else (succ (f (pred x) y))]: o->o
[\x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: (o->o->o)->o->o->o

Type judgement for addRightTerm = \f:o->o->o => \x:o => \y:o => if0 y then x else (succ (f x (pred y)))
[f]: o->o->o
[x]: o
[y]: o
[pred y]: o
[f x]: o->o
[f x (pred y)]: o
[succ (f x (pred y))]: o
[if0 y then x else (succ (f x (pred y)))]: o
[\y:o => if0 y then x else (succ (f x (pred y)))]: o->o
[\x:o => \y:o => if0 y then x else (succ (f x (pred y)))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 y then x else (succ (f x (pred y)))]: (o->o->o)->o->o->o

Type judgement for add = \x:o => \y:o => (fix addLeftTerm) x y
[x]: o
[y]: o
[fix addLeftTerm]: o->o->o
[(fix addLeftTerm) x]: o->o
[(fix addLeftTerm) x y]: o
[\y:o => (fix addLeftTerm) x y]: o->o
[\x:o => \y:o => (fix addLeftTerm) x y]: o->o->o

Type judgement for add' = \x:o => \y:o => (fix addRightTerm) x y
[x]: o
[y]: o
[fix addRightTerm]: o->o->o
[(fix addRightTerm) x]: o->o
[(fix addRightTerm) x y]: o
[\y:o => (fix addRightTerm) x y]: o->o
[\x:o => \y:o => (fix addRightTerm) x y]: o->o->o

Type judgement for mulTerm = \f:o->o->o => \x:o => \y:o => if0 y then x else add x (f x (pred y))
[f]: o->o->o
[x]: o
[y]: o
[pred y]: o
[f x]: o->o
[f x (pred y)]: o
[add x]: o->o
[add x (f x (pred y))]: o
[if0 y then x else add x (f x (pred y))]: o
[\y:o => if0 y then x else add x (f x (pred y))]: o->o
[\x:o => \y:o => if0 y then x else add x (f x (pred y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 y then x else add x (f x (pred y))]: (o->o->o)->o->o->o

Type judgement for mul = \x:o => \y:o => (fix mulTerm) x (pred y)
[x]: o
[y]: o
[pred y]: o
[fix mulTerm]: o->o->o
[(fix mulTerm) x]: o->o
[(fix mulTerm) x (pred y)]: o
[\y:o => (fix mulTerm) x (pred y)]: o->o
[\x:o => \y:o => (fix mulTerm) x (pred y)]: o->o->o

Type judgement for factorial = \f:o->o => \n:o => if0 n then 1 else mul n (f (pred n))
[f]: o->o
[n]: o
[1]: o
[pred n]: o
[f (pred n)]: o
[mul n]: o->o
[mul n (f (pred n))]: o
[if0 n then 1 else mul n (f (pred n))]: o
[\n:o => if0 n then 1 else mul n (f (pred n))]: o->o
[\f:o->o => \n:o => if0 n then 1 else mul n (f (pred n))]: (o->o)->o->o

Type judgement for fact = \n:o => (fix factorial) n
[n]: o
[fix factorial]: o->o
[(fix factorial) n]: o
[\n:o => (fix factorial) n]: o->o

Type judgement for x = 5
[5]: o

Type judgement for y = 3
[3]: o

Type judgement for eval {add x y}
[add x]: o->o
[add x y]: o

Type judgement for eval {catch add'}
[catch add']: o

Type judgement for eval {mul x y}
[mul x]: o->o
[mul x y]: o

Type judgement for eval {fact 5}
[5]: o
[fact 5]: o
\end{minted}

\end{document}
