\documentclass[12pt,a4paper]{report}
\usepackage{Bath-CS-Dissertation}
\usepackage[newfloat]{minted}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usemintedstyle{vs}
\usepackage{caption}

\title{\bf $\langle$Dissertation Title$\rangle$}
\author{Jay Rabjohns}
\date{Bachelor of Science in Computer Science\\ 
      The University of Bath\\
      2024}

% Numberings for Listing
\makeatletter
%\renewcommand*{\thelisting}{\thesection.\arabic{listing}}
\renewcommand*{\thelisting}{\thechapter.\arabic{listing}}
\@addtoreset{listing}{section}
\makeatother

% Numberings for theorems, corollaries, and lemmas
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]%section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\begin{document}
\hypersetup{pageanchor=false}

% Set this to the language you want to use in your code listings (if any)
\lstset{language=Haskell,breaklines,breakatwhitespace,basicstyle=\small}

\setcounter{page}{0}
\pagenumbering{roman}

\maketitle
\newpage

\declaration{$\langle$Dissertation Title$\rangle$}{Jay Rabjohns}
\newpage

\hypersetup{pageanchor=true}
\abstract
$\langle$The abstract should appear here. An abstract is a short paragraph describing the aims of the project, what was achieved and what contributions it has made.$\rangle$
\newpage

\tableofcontents
\newpage

%\listoffigures
%\newpage

%\listoftables
%\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter*{Acknowledgements}
%Add any acknowledgements here.

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
This is the introductory chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature and Technology Survey}
This chapter discusses the literary and historical background that the project relies on. It starts by introducing concepts and discussing their historical significance before ultimately landing on concrete implementation ideas for the project at hand. Some of the topics include:
\begin{itemize}
    \item The $\lambda$-calculus and its computational model
    \item SPCF and other extensions of the $\lambda$-calculus
    \item Nesting removal in SPCF and its theoretical base
    \item Implementation details for an SPCF interpreter
\end{itemize}

\section{The \texorpdfstring{$\lambda$}{lambda}-calculus}
The lambda calculus is an important tool in the field of functional programming. It is an abstract model of computation introduced by \cite{church_1936} which provides compact semantics for studying computation. It is analogous to a simple yet very powerful programming language. While it was originally used to study the foundations of mathematics, specifically the 'Entscheidungsproblem' or 'Decision Problem', it has since been adapted and expanded to accommodate a wide array of domains, including being the basis for functional programming as a whole. It is computationally complete, meaning it can represent any computable function or equivalently it can simulate any Turing machine \cite{turing_1937}.

The lambda calculus is defined by the BNF grammar \eqref{eq:lambda_calc}, which provides an inductive definition for all lambda terms. Each term denotes a function and any term can be applied to any other term. $x$ is one of infinitely many variables, represented as a string. $\lambda x.N$ denotes an abstraction, which are functions that evaluate $M$ by binding all free occurrences of the supplied argument $x$ in $M$. They can be thought of as the suspended execution of a function, allowing it to be composed and reasoned about before its evaluation. $MN$ is the application of an argument $N$ to a function $M$. Together, these provide the basis to construct any valid term in the lambda calculus, highlighting how concise of a definition it has and further making the fact it is Turing complete quite incredible.

\begin{equation}\label{eq:lambda_calc}
    M,N ::= x\ |\ \lambda x.M\ |\ MN
\end{equation}
\subsection{Computation in the \texorpdfstring{$\lambda$}{lambda}-calculus}

Computations in the lambda calculus are usually presented as a series of transformations $M \rightarrow M' \rightarrow M'' \rightarrow \ldots$. The basic computation step is a $\beta$-reduction \eqref{eq:beta_reduction}, where a term $(\lambda x.M)N$ is said to reduce to $M[N/x]$. This means that every occurrence of $x$ in $M$ is substituted with $N$. This is an example of a reducible expression or $\beta$-redex. A normal form is a term to which no further computation can be performed, they are significant because ultimately they provide a convenient way of defining relations between terms. One takeaway from this which may not be immediately obvious is that terms can contain free variables, terms which reference variables not bound by a surrounding abstraction. This will become especially relevant later in Section \ref{sec:nesting_removal} when discussing argument sharing.

\begin{equation}\label{eq:beta_reduction}
   (\lambda x.M)N \rightarrow_{\beta} M[N/x]
\end{equation}

Relations can be defined between terms, denoted by $M \sim N$, which means that $M$ and $N$ are related by $\sim$. It is possible to define equivalence relations for expressing equivalence, which can be thought of as being identical modulo something related to the relation. $\alpha$-equivalence $M =_{\alpha} N$ states that terms are equivalent if they are identical in every respect modulo variable names. $\beta$-equivalence $M =_{\beta} N$ holds when $M \rightarrow_{\beta}^* N$, ensuring that each substitution $M[N/x] =_{\alpha} M$ to avoid variable capture. Probably the most important equivalence relation to the project is observational equivalence $M \simeq N$, where terms are considered to be equivalent if their outputs are indistinguishable for any given input, this will become more relevant in Section \ref{sec:nesting_removal}.

Many familiar higher-level constructs can be defined in the lambda calculus such as booleans, if-then-else expressions, natural numbers, and numerical operations like addition. These constructs allow real computation to be performed in terms of the lambda calculus. Church introduced a series of encodings for these, aptly named the Church encodings; \eqref{eq:booleans} provides definitions for booleans and $ifthen$. It is common for people to define a series of named constants as syntactic sugar for the lambda calculus, making programs considerably more readable while keeping the computational power. An example reduction of $ifthen$ can be seen below.
\begin{equation*}
\begin{split}
ifthen\ true\ M\ N &\rightarrow_{\beta}^* M\\
ifthen\ true\ M\ N &= (\lambda b. \lambda x.\lambda y.bxy)(\lambda x.\lambda y.x) M N\\
& \rightarrow_{\beta} (\lambda x. \lambda y.(\lambda x.\lambda y.x) x y) M N\\
& \rightarrow_{\beta} (\lambda y.(\lambda x.\lambda y.x) M y) N\\
& \rightarrow_{\beta} (\lambda x.\lambda y.x) M N\\
& \rightarrow_{\beta} (\lambda y.M) N\\
& \rightarrow_{\beta} M
\end{split}
\end{equation*}

Oppositely, it can be said that $ifthen\ false\ M\ N \rightarrow_{\beta}^* N$, but the full reduction is omitted here.

The encodings for numerals and associated operators have been omitted for brevity.
\begin{equation}\label{eq:booleans}
\begin{split}
true &= \lambda x.\lambda y.x\\
false &= \lambda x.\lambda y.y\\
ifthen &= \lambda b. \lambda x.\lambda y.bxy
\end{split}
\end{equation}

Recursion is the act of a function referencing itself. In the lambda calculus, recursion is modelled by a so-called 'fixed point' operator, defined by $M =_{\beta} F\ M$, where the input term of a function is equal to its output. Every term in the lambda calculus has at least one fixed point. The fixed point of a term can be found through a so-called 'fixed point combinator', which cleverly uses self-application to deduce the fixed point. Many fixed point combinators exist, one of the simplest is the Y-combinator \eqref{eq:y_combinator} introduced by \cite{curry_1930}. A combinator is simply a closed term, a term with no free variables. As an aside, the constants such as $ifthen$ and $true$ defined earlier are also kinds of combinator.

\begin{equation}\label{eq:y_combinator}
    Y = \lambda f.(\lambda x.f(xx))(\lambda x.f(xx))
\end{equation}

\subsection{Typing in the \texorpdfstring{$\lambda$}{lambda}-calculus}
It is possible and indeed common to extend the lambda calculus by editing its grammar. One such way is the addition of types, creating a typed lambda calculus. Types encode additional information about terms, including how they can be applied to one another. The addition of types is a trade-off, it cannot express all terms from the lambda calculus, however more can be proven about the terms it can express. Generality is lost in favour of specificity.

The simply typed lambda calculus is a form of typed lambda calculus originally introduced by \cite{church_1940} to address what he felt was a paradox in his untyped lambda calculus, self-application. An example of self-application is the term $\lambda x.xx$, commonly denoted $\Omega$, which is commonly used as an example of a diverging program. A grammar for the STLC is given by \eqref{eq:typed_lambda_calc}, where types are either a base type $o$ or constructed from two types $\sigma \rightarrow \tau$. Comparing the grammar to the untyped lambda calculus \eqref{eq:lambda_calc}, we see that the typing rules restrict the terms which can be applied to abstractions, illustrated by binding variables being annotated by a type $\tau$.

\begin{equation} \label{eq:typed_lambda_calc}
\begin{split}
    M,N &::=x\ |\ \lambda x^{\tau} .M\ |\ MN\\
    \tau , \sigma &::= o\ |\ \sigma \rightarrow \tau
\end{split}
\end{equation}

The significance of preventing self-application is that it is said to be strongly normalising, every term terminates and every term has a $\beta$-normal form, shown by \cite{tait_1967}. The trade-off for this is that recursion is no longer possible since self-application is not possible. This can be illustrated by trying to type the Y-combinator \eqref{eq:y_combinator}, where any type assigned to $x$ will always lead to a contradiction of the type for $f$. We will come back to this when reasoning about affine programs in section \ref{sec:nesting_removal}, specifically the differentiation between bounded and unbounded programs.

\section{Extensions of the simply typed \texorpdfstring{$\lambda$}{lambda}-calculus}
\subsection{Logic of Computable Functions}
In 1969 Dana Scott proposed the logic of computable functions (LCF), a logic which serves as the basis for a typed calculus supporting fixed-point recursion. It relies on the concept of a complete partial order (CPO), which is a relation between sets '$\leq$' that captures the idea of ordered computation. More specifically, elements of a set are related by a partial order and it is complete in the sense that each sequence of partially ordered elements has a least upper bound. In the context of fixed points, Scott shows that it is possible to construct a 'least fixed point', which is the smallest solution to a fixed-point equation, which implies that the recursion converges to a solution. Remember that while recursion via the least fixed point converges, it is not implied that the function terminates. For example, a function which always returns a constant value and never terminates is considered to have converged. LCF has laid the groundwork for more complex calculi than the STLC, and indeed Scott later used it to construct his X-Calculus but that is unrelated to the project at hand. This work was originally part of an unpublished note however it has since been published as a memorandum by \cite{milner_1973} to make it more accessible.

\subsection{Programming of Computable Functions}
PCF is a sequential functional language based on LCF introduced by \cite{plotkin_1977}. Whilst LCF is focused on providing a logic for proofs, PCF focuses more on practical computation and looks similar to a real-world programming language. It is, in essence, an extension of the simply typed lambda calculus supporting recursion as well as providing data types and functions out of the box. Its grammar is shown in \eqref{eq:pcf_grammar}, and similar to the STLC types are either a ground type or constructed between two existing types.

\begin{equation} \label{eq:pcf_grammar}
\begin{split}
    M,N &::= n\ |\ f\ |\ x_{\tau}\ |\ (\lambda x_{\sigma} .M_{\tau})_{\sigma \rightarrow \tau}\ |\ (M_{\sigma \rightarrow \tau} \ N_{\sigma})_{\tau}\\
    n &::= 1, 2, 3, \dots\\
    f &::= succ_{o \rightarrow o} \ | \ pred_{o \rightarrow o} \ | \ cond_{o \rightarrow o \rightarrow o \rightarrow o} \ | \ Y_{(\tau \rightarrow \tau) \rightarrow \tau}\\
    \tau , \sigma &::= o\ |\ \sigma \rightarrow \tau
\end{split}
\end{equation}

One of the major challenges relating to PCF has been to create a model which is fully abstract, a crucial property for characterising the observational equivalence of programs. \cite{plotkin_1977} admits that in order for the provided model to be fully abstract, there must exist functions capable of computing more than one argument simultaneously, which contradicts PCF being a sequential language. This is due to the CPO model of PCF being defined over continuous functions despite the language only being able to represent sequential functions. Further models have been developed which provide full abstraction, notably one by \cite{milner_1977}, however these are considered less than satisfactory. Later, \cite{loader_1996} disproved the existence of an \textit{effectively representable} fully abstract model of finitary PCF, which is another important property regarding the definition of observational equivalence. Another approach to the problem is to extend the language rather than to modify its model, enabling the definition of a sequential and fully abstract language. This is what will be discussed in the next section.

\subsection{Sequential PCF}\label{sec:spcf}
SPCF is an extension of PCF developed by \cite{cartwright_1992} which introduces error generators and escape handlers, acting as two kinds of control operators. Error generators describe the misapplication of terms and escape handlers can be thought of as escaping from local evaluation of a phrase.

\begin{equation} \label{eq:spcf_grammar}
\begin{split}
    M,N &:= n\ |\ f\ |\ e\ |\ x^{\tau}\ |\ (\lambda x^{\sigma} .M^{\tau})^{\sigma \rightarrow \tau}\ |\ (M^{\sigma \rightarrow \tau} \ N^{\sigma})^{\tau}\\
    n &= 1, 2, 3, \dots\\
    f &= succ^{o \rightarrow o} \ | \ pred^{o \rightarrow o} \ | \ if0^{o \rightarrow o \rightarrow o \rightarrow o} \ | \ Y^{(\tau \rightarrow \tau) \rightarrow \tau}\\
    e &= error_1^{\ o}\ |\ error_{2}^{\ o}\ |\ catch^{\tau_1 \rightarrow \dots \tau_n \rightarrow o}\\
\end{split}
\end{equation}

\begin{equation}
    \tau , \sigma := o\ |\ \sigma \rightarrow \tau
\end{equation}

\subsubsection{Observing evaluation order with errors}\label{section: observing sequentiality}
Functions in SPCF are error-sensitive, meaning that if an argument evaluates to an error the function also returns an error. This error propagation is analogous to the behaviour of $try...catch$ statements commonly found in practical languages. Error sensitivity allows a programmer to determine the evaluation order of a function's arguments by substituting them with distinct error values and observing which one is thrown. For example, consider two possible recursive definitions of addition in SPCF. One recurses on its first parameter and the other on its second.

\[+_l = Y(\lambda+.(\lambda x.\lambda y.\ if0\ x\ y\ succ (+\ (pred\ x)\ y)))\]
\[+_r = Y(\lambda+.(\lambda x.\lambda y.\ if0\ y\ x\ succ (+\ x\ (pred\ y))))\]

Due to functions being error-sensitive, the programmer can manually apply errors in different orders and observe the changes in behaviour. ($+_l\ error_1\ error_2$) evaluates $x$ first and so it returns $error_1$, ($+_r\ error1\ error2$) evaluates $y$ first and so it returns $error_2$. This behaviour is precisely what makes SPCF \textit{observably} sequential and what ultimately enables the construction of a fully abstract sequential language. 

\subsubsection{Observing evaluation order with catch}
The catch operator is introduced as part of SPCF so that a function's order of evaluation may be determined internally as part of a program. The inclusion of errors allows a programmer to try different combinations of errors as function arguments and remember their result, and with some kind of internal construct, catch, there is now a way for programs themselves to determine this.

\cite{cartwright_1992} introduce a family of catch procedures based on the catch construct found in the original version of Scheme. Catch as defined here is a family of procedures with types ($\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow o$, which is to say that if $f$ is a function with type $\tau_1 \rightarrow \dots \rightarrow \tau_n$, $catch\ f$ will return a base type. $catch\ f$ returns the index of the argument in which it is \emph{strict}, which means the argument which is evaluated first. If $f$ evaluates no arguments and returns a constant, then the plus the number of arguments is returned. This catch procedure is equivelantly expressive to the downward catch defined in Scheme but this is slightly simpler to reason about. 

In Chapter \ref{chapter: denesting}, we will discuss in further detail the implications of including errors and catch in SPCF but for now it is worth knowing that control operators will play a fundamental role in the removal of function nesting, where nested calls are effectively replaced by a series of jumps.

\section{Nesting Removal in SPCF}\label{sec:nesting_removal}
It is incredibly common when writing programs to nest function calls. It leads to terse and expressive code but can lead to complicated implementations. A more formal definition of a nested function call can be defined as the sharing of variables from a function's scope as arguments of another function. It is possible to refactor a program, that is to change its representation to an equivalent form without changing its behaviour, to eliminate this behaviour. \cite{laird_2007} outlines a method for this in SPCF by transforming programs to a sublanguage ASPCF and subsequently projecting it back to the original type. Further, he shows that for every term of SPCF $M$ there exists an ASPCF term $M'$ such that $M \simeq M'$.

ASPCF is SPCF with some additional typing constraints, referred to as affine typing. Affine typing restricts the use of variables and removes recursion, enforcing a form of linearity in programs. Commonly, affine typing implies variables may only be used once, but in this case it means variables may have a single reference or copy in use at any given time. Consequently, terms of an application may not share free variables, preventing functions from interfering with one another's evaluation. For example, for an affinely typed term $(A\ B)\ C$, $B$ and $C$ can contain no common free variables. It is for this reason that fixed point recursion is also removed, clearly the inner applications of the Y combinator \eqref{eq:y_combinator} cannot generally guarantee that terms contain no common free variables.

\cite{laird_2007} first considers a bounded version of SPCF where types are finite, for example booleans or a bounded subset of the natural numbers. As well as this, terms are not recursive. What follows are denotational semantics creating a pair of injection and projection relations between SPCF terms and observationally equivalent ASPCF terms. Secondly, an unbounded call-by-value SPCF is considered. Here, numerals are defined over the natural numbers and terms may have recursive definitions. To not violate affine typing rules, recursion is replaced with iteration. From this, it is shown that unbounded terms have an affinely typed observationally equivalent term, and hence the same method is also applicable to unbounded terms.

While it may be possible to refactor unbounded terms, in the real world, it makes little sense to implement and run this process naively for non terminating programs. The refactoring process would also be unbounded, both in execution time and memory usage. One potential usecase is to refactor terms in parts, considering that some paths in an unbounded program may still be bounded and it would be possible to remove nesting for these parts.

The transformation is wholesale, meaning entire branches of the program would have to be considered at once. Importantly it's not possible to denest a function in isolation without also denesting its dependants. The process would have to be done to entire branches or paths of a program at once. A program path is a common conceptualisation of decisions being made in the program, for example conditionals and function calls both fork the program into multiple paths. From this, it is theoretically possible to lazily refactor paths of a program and over time return these to the user. This approach would of course also work for the bounded case, where it would terminate as before rather than having one or many long-standing or potentially infinite tasks. 

\section{Implementation}\label{section: lit-review-impl}
An appropriate representation for SPCF programs in computer memory is needed. This will most likely be a form of abstract syntax tree (AST), which are commonly used in language compilers and parsers. They represent programs as a hierarchical tree of statements which follow the same syntactic ordering of the original program. This provides a convenient and efficient representation to operate on. The AST could be constructed either through tokenising an input string or by hard coding it. For this project we provide methods of doing both.

A very natural way to model an AST is through the use of an algebraic data type (ADT), which is supported well by Haskell. An ADT provides an expressive way to define and combine product types, such as structs and tuples, as well as sum types, such as enums. Their expressiveness and ability to model recursive types make them a natural fit to model inductive definitions such as the grammar for SPCF and recursive data structures like ASTs. \cite{jones_2003} discusses idioms related to abstract syntax tree implementation and agrees that functional languages provide a natural basis for them because of their support for user-defined recursive datatypes. For this interpreter, variables, abstractions, and applications will each have a corresponding representation in the ADT. Constants such as $succ_{o \rightarrow o}$ will not be a part of the ADT but rather functions to construct their relevant terms.

Once an AST is constructed, it will be possible to evaluate terms by implementing the small-step operational semantics set out by \cite{laird_2007}. Each step will be a function which pattern matches on the term's ADT, applying the relevant reduction rule.

In regards to implementing the denesting action, \cite{laird_2007} denotationally outlines the injective and projective mappings between SPCF and ASPCF terms which should be straightforward enough to implement once there is a working representation of SPCF in computer memory. 

\subsection{Testing and Evaluation}
To test the interpreter works, nested and denested terms will be evaluated on arbitrary data and the results will be compared. While term evaluation is still under development, it will be possible to do the evaluations by hand to get the same effect. For bounded terms, it could be possible to map every input and output for the SPCF term and check that they are equivalent to that of the ASPCF term. This should hold since they are observationally equivalent. 

Unit tests could be used for the lower level language concepts such as reduction and term construction.

It could be possible to recreate the proofs set out by \cite{laird_2007} but that may incur a lot of additional work, so we will most likely stick to unit tests and comparison to by hand evaluation to provide an approximation of correctness commonly accepted in software development.

\section{Conclusion}
Laird provides a valuable framework for function nesting removal in SPCF. By tracing the rich historical lineage of SPCF from PCF, LCF, and the lambda calculus, we contextualise function denesting within a broader scope. We discuss some potential applications of unbounded function denesting, as well as ideas for implementation. Haskell is a fitting language for implementation due to its support of algebraic datatypes and lazy evaluation. With a well-defined basis, the project's implementation is a natural next step.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{SPCF Interpreter}\label{chapter: interpreter}
This chapter introduces the prerequisite tools and foundational concepts material needed for Chapter \ref{chapter: denesting}, which discusses the denesting action in detail. This action relies on a wide range of concepts, such as continuation passing style programming, fixed point recursion, and various approaches to typing. We aim to build on Section \ref{section: lit-review-impl} while including notes on implementation.

To effectively showcase the denesting action, we require a suitable representation for SPCF which can be manipulated and reasoned about in computer memory. Additionally, one of the most effective approaches to test its implementation is to evaluate terms pre and post-transformation transformation, ensuring their behaviour is unchanged, which is one of the transformation's key properties. For this reason, we have developed an interpreter for SPCF, complete with a frontend for parsing programs in plaintext. At the time of writing, there are no widely available interpreters for SPCF, meaning that this is done mostly out of necessity. However, this also presents an excellent opportunity to discuss interesting features of SPCF and highlight design decisions required for implementing the denesting action in chapter  \ref{chapter: denesting}.

\section{Execution model}\label{section: language representation}
An adequate runtime execution model is required if we are to construct and evaluate terms in SPCF. Haskell is a popular choice for building interpreters for variants of the $\lambda$-calculus and a common approach to representing terms is as an algebraic data type (ADT). This is illustrated in Listing \ref{listing:spcf-ast}, each term of the grammar \eqref{eq:spcf_grammar} is represented as a constructor. In terms of abstract syntax trees, leaf nodes represent variables and constants whereas branches are applications, abstractions, and functional constants. 

Other variants of lambda calculus share many aspects with SPCF, such as variables, abstractions, applications, and common function constants such as the successor and predecessor. What is in some senses interesting about SPCF is both the typeable fixed point combinator borrowed from LCF and its inclusion of a non-local operator 'catch'. 

\begin{listing}
\caption{SPCF AST inductive definition using an ADT}
\begin{minted}{haskell}
data Term
  = Numeral Int
  | Error Error
  | Variable Label
  | Lambda Label Type Term
  | Apply Term Term 
  | Succ Term
  | Pred Term
  | YComb Term
  | If0 Term Term Term
  | Catch Term
\end{minted}
\label{listing:spcf-ast}
\end{listing}

\subsection{Capture avoiding substitution}
Similarly to any other variant of the $\lambda$-calculus, SPCF assumes well-defined definitions for $\alpha, \beta, \eta$ equivalence, and of course, capture avoiding substitution. Our implementation includes these, with the notable omission of $\eta$-expansion because it is not relied upon by other operations. Capture avoiding substitution is often glossed over when discussing abstract language definitions, however seeing as our implementation is concerned with a concrete language definition, we shall briefly discuss it.

Central to capture avoiding substitution is the notion of a fresh variable. Fresh variables are valid variable names which are not already bound in a term. We use Haskell's lazy evaluation to construct an infinite list of variable names of the following form: 
\[
\begin{split}
&vars := \{"a", \dots, "z"\} \cup \{\ ci\ |\ c \in \{"a", \dots, "z"\}, i \in \mathbb{N}\} \\
&\text{where $ci$ is the concatenation of $c$ and $i$.}
\end{split}
\]
A fresh variable is then chosen by taking the first element of the set which is not already used in the current term. This method guarantees the existence of a fresh variable and minimises computational overhead as it is lazy. Then, after being sure to rename bound instances of the given variable to the fresh variable, substitution can be carried out as normal.

\subsection{Bounded and Unbounded SPCF}
We introduce a bounded variant of SPCF which will serve as a basis for the denesting action working on bounded terms later. This is achieved primarily by removing fixed point recursion and limiting numerals to be smaller than an upper bound $n$, but some other simplifications have been made such as removing if0, succ, pred, and error constants. Additionally, it is extended with binary products, $n$-fold products, and the conditional operator 'case'.  

\begin{equation} \label{eq:bounded_spcf_grammar}
\begin{split}
    M,N &:= B\ |\ F\ |\ x^{\tau}\ |\ (\lambda x^{\sigma} .M^{\tau})^{\sigma \rightarrow \tau}\ |\ (M^{\sigma \rightarrow \tau} \ N^{\sigma})^{\tau}\ | \ M \times N\\
    B &= \{\underline{n}\ |\ n\in \mathbb{N}\}\\
    F &= \text{catch}_{m}^{\ \tau_1 \rightarrow \dots \tau_n \rightarrow o}\ |\ \text{case}^{o \rightarrow o^n \rightarrow o}\\
\end{split}
\end{equation}

\begin{equation}
    \tau , \sigma := o\ |\ \sigma \rightarrow \tau\ |\ \sigma \times \tau\ |\ \underline{0}
\end{equation}

\textbf{Products}

This definition relies on a suitable encoding for binary as well as $n$-fold products along with projections $\pi_i : \tau^n \Rightarrow \tau$ and $\pi_1 : \tau \times \sigma \Rightarrow \tau$, $\pi_2 : \tau \times \sigma \Rightarrow \sigma$. $n$-fold products provide a convenient syntax for constructing long collections of terms of uniform type. We provide an implementation of n-fold products by abstracting over a list and making use of its built-in functions for traversal and indexing. The empty product $I$ is simply the empty list.

\section{Types}\label{section: typing}
Types encode additional information about terms, including how they can be applied to one another. Similarly to other typed lambda calculi, types in SPCF can be encoded as either a 'base' type $o$ or an 'arrow type' $\sigma \rightarrow \tau$. This set has been expanded to include an explicit 'Cross' type to encode n-fold and binary products as well as an 'Empty' type to typify non-terminating programs, which is necessary for programs written in continuation passing style, which the denesting action relies upon.

\begin{listing}
\caption{SPCF inductive definition for types using an ADT}
\begin{minted}{haskell}
data Type
  = Base            -- Base type (numerals and errors)
  | Empty           -- Return type of a non-terminating function
  | (:->) Type Type -- Function
  | Cross Type Type -- Product
\end{minted}
\label{listing:type-adt}
\end{listing}

\begin{definition}
    A typing \emph{context} $\Gamma$ is a map of variables to types.
\end{definition}

A typing context can be represented as a list, for example, $\Gamma = a:\tau, b:\sigma, \ldots$. It is also common to extend contexts, for example, $\Gamma, x:\tau$ reads as the context $\Gamma$ extended with $x$ having type $\tau$. 

\begin{definition}
    A typing \emph{judgement} $\Gamma \vdash M:\tau$ is a proposition, stating that given a context $\Gamma$, the term $M$ has type $\tau$. A judgement can be true or false, depending on the given context.
\end{definition}

\begin{definition}
    A typing \emph{rule} determines if it is possible for a term to have a type, sometimes this is called being well-typed. Rules are written like this
    \[
    \frac{Premise_1 \quad\quad Premise_2 \quad\quad \ldots \quad\quad Premise_n}{Conclusion}
    \]

    This can be read as 'given each of the things on the top, the thing on the bottom must be true'. It is possible to have a rule with nothing on the top, which is similar to an axiom or a statement which is always true.
\end{definition}

\subsubsection{Typing rules} \label{section: type rules}

\textbf{Variables}
\begin{equation}
    \frac{}{\Gamma, x:\tau \vdash x: \tau}
\end{equation}

\textbf{Abstraction}
\begin{equation}
    \frac{\Gamma, x:\sigma \vdash M : \tau}{\Gamma \vdash \lambda x^{\sigma}. M : \sigma \rightarrow \tau}
\end{equation}

\textbf{Application}
\begin{equation}
    \frac{\Gamma \vdash M : \sigma \rightarrow \tau \quad\quad \Gamma \vdash N : \sigma}{\Gamma \vdash MN : \tau}
\end{equation}

\textbf{Product}
\begin{equation}
    \frac{\Gamma \vdash M:\sigma \quad\quad \Gamma \vdash N:\tau}{\Gamma \vdash \langle M,N \rangle : \sigma \times \tau}
\end{equation}

\textbf{Empty Product}
\begin{equation}
    \frac{}{\Gamma \vdash \langle \rangle : I}
\end{equation}

\textbf{Projection}
\begin{equation}
    \frac{\Gamma \vdash M:\tau_1 \times \tau_2}{\Gamma \vdash \pi_i M: \tau_i} i \in \{1, 2\}
\end{equation}

\subsection{Implementation details} 
There are two primary approaches to interpreting typing rules depending on the order in which layers are read. If read top to bottom, rules describe a type verification algorithm which ensures terms adhere to type rules. If read from bottom to top, they describe a type inference algorithm capable of determining the type of a given term. Both methods traverse a term's syntax tree applying type rules at each sub-expression and both rely on the fact that every term in SCPF has a unique type.

For this project, a type inference algorithm is necessary to construct a general application of the denesting action described in Chapter \ref{chapter: denesting}. The action relies on the type structure of terms it is applied to and in some senses describes a family of actions, each of which acting on terms of a specific type. The tree traversal and book keeping of variables is done in much the same way as for term evaluation, described in Section \ref{section: evaluation}, with the notable addition of applying the type rules specified in \ref{section: type rules}.

One might wonder why the implementation of the denesting action, which ultimately takes the form of a pair of terms in the language, is written in Haskell and not in SPCF. That is to say, if our implementation is fully expressive why must we rely on Haskell to construct these terms? The answer lies once again as a typing problem. We have described the typing system of SPCF as monomorphic, where each term has a unique type and the type inference algorithm is similar to the original definition by \cite{curry1958combinatory}. To define a term across many types as would be needed here we would require a type system supporting parametric polymorphism. Without this, it is possible to exhaustively define pairs of terms for a finite subset of types, but nothing more. This would be possible and indeed type inference algorithms for these type systems exist \cite[\emph{e.g.},][]{hindley_1969, milner_1978} however they are more complex. Therefore we utilise Haskell's more advanced type system to define these terms generically without the additional implementation complexity. 

\subsection{Examples}
Consider $+_l$ defined in Section \ref{sec:spcf}, its typing judgement would look like \ref{listing: addLeft judgement}. The program output for the typing of this term is illustrated in Listing \ref{listing: addLeft judgement}. More typing terms and their typing judgements can be found in Appendix \ref{appendix: term evaluations}.
\begin{listing}
\caption{Program output from the typing of $+_l$}
\label{listing: addLeft judgement}
\begin{verbatim}
Type judgement for addLeftTerm = \f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))
[f]: o->o->o
[x]: o
[y]: o
[pred x]: o
[f (pred x)]: o->o
[f (pred x) y]: o
[succ (f (pred x) y)]: o
[if0 x then y else (succ (f (pred x) y))]: o
[\y:o => if0 x then y else (succ (f (pred x) y))]: o->o
[\x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: (o->o->o)->o->o->o

Type judgement for add = \x:o => \y:o => (fix addLeftTerm) x y
[x]: o
[y]: o
[fix addLeftTerm]: o->o->o
[(fix addLeftTerm) x]: o->o
[(fix addLeftTerm) x y]: o
[\y:o => (fix addLeftTerm) x y]: o->o
[\x:o => \y:o => (fix addLeftTerm) x y]: o->o->o
\end{verbatim}
\end{listing}

\section{Term Evaluation}\label{section: evaluation}
Typically when reducing expressions it is tempting to only consider closed terms, terms with no free variables. However, to define rigorous operational semantics, we must consider all scenarios. It is common to represent the rules for reducing expressions as a series of 'small-step' operational semantics, which provide rules on what to do for a given term. Each step could be applied at any point in the computation and as such there could be free variables to account for. A common way of defining these rules is through the use of evaluation contexts.

It is worth noting that not all calculi require a notion of evaluation contexts to define evaluation. In a more classical example of a simply typed lambda calculus, basic datatypes are cleverly represented through a series of higher-order functions, called Church encodings. In that example, a term's computation can be defined as its $\beta$-reduction. It is our more complex language definition which calls for the need for evaluation contexts when defining its evaluation rules.

\begin{definition}
    A \emph{closure} $(E, t)$ is an environment $E$ paired with a term $t$ such that the environment is defined for all free variables in $t$.
\end{definition}

\begin{definition}\label{def: environment}
    An \emph{environment} $E$ maps labels to closures. $E$ \emph{interprets} a label $x$ if there exists a map from $x$. The result to which $E$ interprets $x$ is written as $E[x]$.
\end{definition}

\begin{definition}
    A closure \emph{evaluates} $(E, t) \Downarrow v$ to value $v$ if there exists a value $v$ such that $M \rightarrow^* v$.
\end{definition}

Evaluation contexts are, in essence, an environment and a 'hole' $E[\_]$ which can be filled by any term. When a hole is filled, all the terms free variables take on values from the environment. This naturally describes a way of providing inputs or starting values to a program, as well as equally describing the tracking of an ongoing computation through recording the current values of variables in the environment. This idea of capturing the current state of a computation in a higher kinded type is common in functional programming and is an idiomatic use case for a monad.

A monad represents computations which can be composed together to form new computations. In practice, they are commonly used to track state or to encode the possibility of computational failure. Although monads are a useful abstraction over composable computations, frustratingly they themselves are not composable with one another. To encode multiple monadic effects for a single computation, one must build what is known as a monad transformer, effectively a monad over other monads.

Our implementation of term evaluation in Haskell revolves around a monad transformer we have defined called Eval. It is composed of predefined monads from the Haskell standard library. As illustrated in Listing \ref{listing: eval} it uses a 'Reader' monad to keep track of the current environment, an 'Except' monad to indicate that computation could fail as a string, and a 'Writer' monad to keep track of any supplementary logs which are a useful way of recording the order of operations to look at later. An environment is just a map of labels to terms, exactly as it is defined in Definition \ref{def: environment}. Using this monad, it is possible to define an 'eval' function with the signature below:
\mint{haskell}|eval :: Term -> Eval Term|
This reads as 'eval is a function which maps a term to the Eval of that term'. 

\begin{listing}
\caption{The Eval monad used for term evaluation}
\begin{minted}{haskell}
type Environment = Map.Map Label Term
type Eval a = (ReaderT Environment (ExceptT String (WriterT [String] Identity))) a
\end{minted}
\label{listing: eval}
\end{listing}

Recall that Term (Listing \ref{listing:spcf-ast}) is an algebraic data type, meaning that eval must provide a case for each possibility of what that term could be. This is called exhaustive pattern matching and is commonly used alongside monad transformers to build interpreters, \citep[\emph{e.g.},][]{Liang1995MonadTA}. Each matched pattern corresponds to an evaluation rule of the language, these can be found in Section \ref{section: op sem}.

\subsection{Operational semantics}\label{section: op sem}
Evaluation rules are defined similarly to typing rules and together form a small-step operational semantics.

\textbf{Constants}
\begin{equation}
    \frac{}{E[n\in\mathbb{N}] \Downarrow n}
\end{equation}

\begin{equation}
    \frac{}{E[error_i] \Downarrow error_i}\quad(i \in \{1,2\})
\end{equation}

\textbf{Variables}
\begin{equation}
    \frac{E[x] \Downarrow (E', x') \quad\quad E'[x'] \Downarrow V}{E[x] \Downarrow V}
\end{equation}

\textbf{Abstraction}
\begin{equation}
    \frac{}{E[\lambda x. M] \Downarrow E[\lambda x. M]}
\end{equation}

\textbf{Application}
\begin{equation}
    \frac{E[M] \Downarrow (E', \lambda x. x') \quad\quad  E[N] \Downarrow V' \quad\quad E'\cup \{x\mapsto V'\}[x'] \Downarrow V}{E[MN] \Downarrow V}
\end{equation}

\textbf{Successor}
\begin{equation}
    \frac{E[M] \Downarrow n}{E[succ\ M] \Downarrow n + 1}
\end{equation}

\textbf{Predecessor}
\begin{equation}
    \frac{E[M] \Downarrow n + 1}{E[pred\ M] \Downarrow n}
\end{equation}

\textbf{If0}
\begin{equation}
    \frac{E[P] \Downarrow 0 \quad\quad E[M] \Downarrow V}{E[\text{if0}\ P\ \text{then}\ M\ \text{else}\ N] \Downarrow V}
\end{equation}

\begin{equation}
    \frac{E[P] \Downarrow n \quad\quad n > 0 \quad\quad E[N] \Downarrow V}{E[\text{if0}\ P\ \text{then}\ M\ \text{else}\ N] \Downarrow V}
\end{equation}

\textbf{Y combinator}
\begin{equation}
    \frac{}{E[\text{Y} \lambda x. M] \Downarrow E[M[\text{Y} \lambda x. M / x]]}
\end{equation}


\textbf{Catch}
\begin{equation}
    E[\text{catch}\ M] \Downarrow i \text{ Where i is where M is strict. See Section \ref{section: catch}.}
\end{equation}

\subsubsection{Bounded term evaluation}
For the bounded version of SPCF there are additional evaluation rules needed for products and the conditional statement case. 

\subsection{Observing sequentiality with Catch}\label{section: catch}
One very important and helpful insight into the behaviour of a program is the order in which it evaluates its arguments. It may not be immediately obvious but armed with that knowledge it is possible to completely define a procedure's behaviour. One way for programmers to determine a procedure's evaluation order is by using errors, by applying an error in different positions it is possible to observe which position leads to an error.

SPCF forces all procedures to be error-sensitive, meaning that if an error is evaluated it is immediately returned. For example, $+_l$ $error1$ $error2$, using $+_l$ defined in Section \ref{section: observing sequentiality}, would return $error1$. The inclusion of catch into SPCF allows programs to observe evaluation orders. \cite{cartwright_1992} define it as a family of procedures with types $(\tau_1 \rightarrow \dots \tau_n \rightarrow o) \rightarrow o$ where if $f$ is a procedure of type $\tau_1 \rightarrow \dots \tau_n \rightarrow o$, then catch $f$ returns $i - 1$ if f is strict in its $i$th argument and $k+n$ if $f$ returns a constant. 

In our implementation of Catch, the AST continues to be walked, but in a slightly different way. Like before function constants and applications essentially continue the downward traversal of catch, being careful to evaluate applications and Ifzero in sequential order of their arguments. Things become more interesting when evaluating a lambda abstraction. The label of the abstraction is pushed to a stack maintained by the catch function before continuing downwards. When a variable is reached, the index of the last occurring instance of the label in the stack is returned. If an error is reached, that error is returned, and if a numeral constant is reached,  that value along with the length of the current stack is returned. Catch was originally defined in terms of a lexically scoped control stack, which equally describes this implementation.

Catch encodes non-local control into SPCF programs, meaning it is possible to stop a term's execution prematurely and jump to somewhere else in the program. This feature is not normally present in $\lambda$-calculi and is one of the things which uniquely identifies SPCF from its peers. We discuss further the implications of the existence of catch in Chapter \ref{chapter: denesting}, along with how it is used to construct the affinely definable denesting transformation.

\subsection{Examples}

\section{Parser}
As part of the project, we have provided a lexer and parser for SPCF to make experimentation with the language easier and to bring it closer to practical programming languages. A lexer and parser make up what is known as a compiler frontend and together they enable writing programs as plain text with some syntax sugar, making programs significantly easier to read and write. The execution pipeline looks like this:
\[\text{Lexical analysis -> Parse tokens -> Object code -> Execute object code}.\]
\subsection{Lexical analysis}
Lexical analysis, also called tokenisation, describes the decomposition of source code to a stream of tokens. Using an incredibly simple example, the code `\lstinline{x = 5;}` could be decomposed into the tokens ["x", "=", "5", ";"]. We are effectively classifying substrings according to what they do. Patterns for these tokens are defined ahead of time along with the language definition. It is this list of tokens which is used by the parser to construct an AST. The action of \emph{lexing} is conceptually similar to applying a series of nested regular expressions.

Tokenisation is a well-defined problem and general tools exist to the make the job easier. Alex\footnote{https://haskell-alex.readthedocs.io/en/latest/about.html} is a code generation tool used to build lexers in Haskell. It is configured with a set of regular expressions, which can be constant strings, which have a one-to-one correspondence with a token. Tokens are defined as entries in an algebraic datatype which can be referenced from other parts of the compilation pipeline. Once configured, Alex will generate Haskell code for a lexer defined on those expressions and bundle it with the interpreter binary. It works similarly to a large macro, abstracting many mundane details 
which would otherwise have to be considered if handwriting a lexer, while highlighting the important ones such as which tokens exist.

One benefit of Alex being built with Haskell is that it has extraordinary support for functional idioms, such as using a State Monad for tracking token information during tokenisation. This monad keeps track of metadata such as the file name and current character position, which is used to make errors more informative. 

Using the previous example along with the minimal Alex configuration in Listing \ref{listing: alex config}, it is possible to tokenise the expression \lstinline{x=5;} as the list of tokens \lstinline{[(TokenId "x"), TokenEquals, (TokenNat 5), TokenSemicolon]}. If the expression was malformed such as \lstinline{x@ = 5;}, then the error will include information from the state monad
\begin{verbatim}
spcf: user error (programs/program.spcf:1:2: lexical error at character '@')
\end{verbatim}

\begin{listing}
\label{listing: alex config}
\caption{Minimal Alex configuration to lex assignment operations.}
\begin{verbatim}
-- The top half configures regular expressions to be used when matching tokens
$digit = 0-9
$alpha = [A-Za-z]
tokens :-
  $digit+                       { lex (TokenNat . read) }
  \=                            { lex' TokenEquals }
  \;                            { lex' TokenSemicolon }
  $alpha [$alpha $digit \_ \']* { lex (TokenId) }
{
\end{verbatim}
\begin{minted}{haskell}
-- The bottom half configures the ADT tokens will inhabit
-- A token is a pair consisting of the Alex State monad and the TokeClass ADT.
-- This is to improve error messages.
data TokenClass = TokenId String | TokenSemicolon | TokenNat Int | TokenEquals
data Token = Token AlexPosn TokenClass
\end{minted}
\begin{verbatim}
}
\end{verbatim}
\end{listing}

\subsection{Parsing} 
A parser uses a set of recursive rules to construct the program AST from the stream of tokens generated during lexical analysis. Similarly to tokenisation, parsing is a well-defined problem and tools exist to abstract laborious details. Happy\footnote{\url{https://haskell-happy.readthedocs.io/en/latest/}} is a code generation tool used to build parsers in Haskell, it is based on the famous parser-generator Yacc, which notably has a syntax reminiscent of BNF grammars. Similarly to Alex, Happy configuration can be viewed as a macro, only slightly more complex. It can be read top-down as a series of rules, where each rule may be defined inductively in terms of other rules. 

Using the same example from before and the minimal Happy configuration in Listing \ref{listing: happy config} the expression \lstinline{x=5;} could become encoded in the intermediate AST representation as: 
\begin{minted}{haskell}
Declare "x" (Numeral 5)
\end{minted}

Similarly to Alex, a State monad is used to capture information from parsing to improve error messages. Notice that the pattern matching cases in Listing \ref{listing: happy config} are not exhaustive, if no suitable rule is matched then a parse error is thrown and the programmer is alerted to where the error is. For example the program \lstinline{x = ;} throws the error
\begin{verbatim}
spcf: user error (programs/program.spcf:1:5: parse error at token ';')
\end{verbatim}

As a testament to how powerful Happy and Alex are when used together, both the lexer and parser for the Glasgow Haskell Compiler (GHC) are entirely defined using them\footnote{\url{https://github.com/ghc/ghc/blob/271a7812cbb47494c74b1dc3b7d2a26fd8d88365/compiler/GHC/Parser/Lexer.x}}. 

\begin{listing}\label{listing: happy config}
\caption{Minimal Happy configuration to parse assignment expressions}
\begin{verbatim}
-- Atomic terms have a one to one correspondence with a term in the AST.
-- Map TokenId to AST.Variable
-- Map TokenNat to AST.Numeral 
ATerm :
  | natVal { case $1 of Token info (TokenNat n) -> SPCF.AST.Numeral info n }
  | id { case $1 of Token info (TokenId id) -> SPCF.AST.Variable info id }

-- In this case, statements may only bind values to names.
-- Map TokenId to a declaration using what is defined in `Binder`.
Statement : id Binder { 
    case $1 of Token info (TokenId id) -> SPCF.Interpreter.Declare info id $2 }

-- Determine what can be on the RHS of declarations
Binder : '=' ATerm { $2 }
\end{verbatim}
\end{listing}

\begin{listing}
\caption{Addition defined in SPCF}
\begin{minted}{haskell}
addLeftTerm = \f:Nat->Nat->Nat => 
              \x:Nat => 
              \y:Nat => if x then y else (succ (f (pred x) y));
add = \x:Nat => \y:Nat => (fix addLeftTerm) x y;
eval (add error1 error2);
\end{minted}
\end{listing}

\section{Correctness Testing}


\section{Conclusion}
The goal of implementing the SPCF denesting action defined by Laird relies on a large amount of background material and prerequisite tools working together. We supply a suitable runtime representation for SPCF and a method for evaluating terms. Building upon this, we use code generation tools to create an interpreter frontend for the language, allowing for easier experimentation and bringing it closer to practical programming languages. We discuss a suitable approach to typing, emphasising how the denesting action requires us to prefer type inference over type checking. Particular focus is given to the catch term as it represents a feature not commonly present in other $\lambda$-calculi and requires some nuance when implementing it from scratch. At each stage, points are reinforced with definitions and examples where necessary.

In the next Chapter, we discuss in more detail the denesting action and implications of its implementation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Affinely definable denesting transformation}\label{chapter: denesting}
Up to this point, we have explored SPCF and its predecessors in Section \ref{sec:spcf}, discussed concrete language definitions in Section \ref{section: language representation}, how they materialise in the form of an interpreter in Section \ref{section: evaluation}, and alluded to a refactoring transformation for programs written in SPCF to an equivalent affine form in Section \ref{sec:nesting_removal}. 

Laird originally split the problem into two cases, the transformation of bounded and unbounded terms. Bounded terms are restricted to numerals smaller than some upper bound $n$ as well as not being recursive. Conversely, unbounded terms may use anything in the language definition given that iteration is used in favour of recursion.

In this chapter, we elaborate on the refactoring action, how it uses SPCF-specific features, and comment on its implementation details. We regularly comment on specific details relevant to bounded and unbounded terms individually. Remember, this transformation aims to construct observationally equivalent terms that adhere to affine typing rules. There are two primary components to this, an injection to an intermediate 'tuple representation' consisting of $\top$ and $\bot$ elements, and a projection to recover it again.

\subsection{Continuation passing style}
CPS is a style of programming where all function calls are tail calls. It is possible to have an entry point to the program return the empty type $0$ and all computation be through the calling of continuations.

\begin{definition}
    A \emph{continuation} is a higher-order function representing the current state of a computation.
    
    Classically, continuations have one argument, the value of the computation so far, and return the final result of the computation when the rest of the program has also finished. They are analogous to callback functions or partial function applications. 
\end{definition}

In some senses, CPS programming is a natural opposite of classical functional programming. With the latter, all computations are effectively the return values of functions, whereas with CPS computations are function side effects captured by running continuations. The transformation relies on the notion of continuation passing as its conceptual base case. It is possible to rewrite any function to an equivalent curried function written in CPS, then this transformation may be applied.

A valid question is what curried functions written in CPS look like, consider an example term with the type $bool \times bool \Rightarrow \underline{0}$
\[
\begin{split}
\lambda x^{bool}. \lambda y^{bool}. &\text{if } x  \\
&\text{then }( \text{if } y \text{ then } \bot \text{ else } \bot )\\
&\text{else } \bot
\end{split}
\]

No information can be garnered from the term's return value, it is always the same, but clearly, it may have different runtime behaviours depending on its inputs. For instance, if x is ff, y is never evaluated. This captures the essence of CPS programming.

\section{Procedures as decision trees}
Graphs often encapsulate certain aspects of a program or function's behaviour. These can be at any level of abstraction, such as a high-level data flow visualisation of neural networks, or the low-level control flow of a function's possible execution paths at runtime. The inclusion of a non-local control and error sensitivity in SPCF means that its graph model for functions has more structure than generic function graphs. Intuitively, it makes sense that the graph model for a sequential language should have to encode some essence of sequentiality, specifically the evaluation order of function arguments. This is exactly the case and one of the key findings of \cite{cartwright_1992}, was a denotational model for continuous functions in SPCF constructed from decision trees. 

Consider the term:
\[\lambda xyz.\ \text{if0}\ x\ \text{then}\ (succ\ y)\ \text{else}\ (pred\ z)\]
It is strict in its first argument, and the value of x determines the strictness of its second and third arguments. In the case x is 0, y is evaluated, and otherwise, z is evaluated. It is possible to entirely represent this function in terms of the index at which it is strict, along with a mapping of all possible values of the strict argument to the return value of the function; this is written as the pair $\langle i, f \rangle$. Functions in SPCF are error sensitive and as such error1 and error2 will always map to error1 and error2 respectively. Since functions are continuous, $\bot$ always maps to $\bot$. Thus the function above can be visualised as the decision tree in \eqref{decision tree}.
\begin{equation}\label{decision tree}
\langle 1, 
\begin{cases}
    \bot \mapsto \bot\\
    \text{error}_1 \mapsto \text{error}_1\\
    \text{error}_2 \mapsto \text{error}_2\\
    0 \mapsto\ \langle 2 \begin{cases}
        \bot &\mapsto \bot\\
        \text{error}_1 &\mapsto \text{error}_1\\
        \text{error}_2 &\mapsto \text{error}_2\\
        0 &\mapsto 1\\
        1 &\mapsto 2\\
          &\dots
    \end{cases} &\rangle\\
    1 \mapsto\ \langle 3 \begin{cases}
        \bot &\mapsto \bot\\
        \text{error}_1 &\mapsto \text{error}_1\\
        \text{error}_2 &\mapsto \text{error}_2\\
        0 &\mapsto \bot\\
        1 &\mapsto 0\\
        2 &\mapsto 1\\
          &\dots
    \end{cases} &\rangle\\
    \dots
\end{cases} \quad\quad\rangle    
\end{equation}

Vertices of the tree are the set of pairings  $\langle i, f \rangle$ and the edges leading from each vertex are the set of possible values for argument $i$. 

Other models for SPCF exist, for example, \cite{kanneganti_1993} provide an alternate model, reconstructed from this one, using the idea of a prime basis \citep{Winskel1980EventsIC} in an attempt to highlight other aspects of SPCF. We focus on this original decision tree model because the mental model of functions being equivalent to some branching structure is exceedingly helpful when reasoning about the denesting action discussed in the next section.

\section{Procedures as tuples}\label{section:inj-proj}
The basis of the transformation of programs into an equivalent affinely typeable form set out by Laird is a pair of injection and projection functions. The injection maps a term to a tuple of the index at which it is strict and a collection of continuation functions for each possible applicable value. 

\begin{definition}
    The insertion of a value $x$ at the position $i$ into a given tuple $t = \langle t_0, \dots, t_{n-1} \rangle$ where $i \leq n$ is denoted $t\lfloor x \rfloor_i$. For example $t\lfloor x \rfloor_i = \langle t_0, \dots, t_{i-1}, x, t_i, \dots, t_{n-1} \rangle$
\end{definition}

\begin{definition}
    The removal of a value $x$ at the position $i$ from a given tuple $t = \langle t_0, \dots, t_{n-1} \rangle$ where $i \le n$ is denoted $\lceil t \rceil_i$. For example $\lceil t \rceil_i = \langle t_0, \dots, t_{i-1}, t_{i+1}, \dots, t_{n-1} \rangle$
\end{definition}

\subsubsection{Injection to tuple form}
\begin{definition}
    The function \emph{inj}, which transforms a term in the bounded SPCF to a so-called 'tuple form', is definable as \\
    inj: $\llbracket \underline{n}^{m+1} \Rightarrow \underline{0} \rrbracket \rightarrow \llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \rrbracket$\\\\
    inj($f$) =
    $\begin{cases}
        f &\text{if}\ f \in \{\top, \bot\}\\
        \langle i, \langle f'_j\ |\ j < n \rangle \rangle\ &\text{if}\ i = \text{catch } f  \text{, where } f'_i(x) =  f(x \lfloor j \rfloor_i)
    \end{cases}$\\\\
    Here $\top$ is interpreted as an unrecoverable error, taking the place of error constants $error_1\ \&\ error_2$. 
    This is definable as the affinely typeable term:
    \[
    \lambda f.\text{case}\langle \text{catch}\ f, \langle \langle j, \langle \pi_0(x),\dots,\pi_{m-1}\rangle \lfloor j \rfloor_i\ |\ i \leq m \rangle \rangle\ |\ j \le n \rangle
    \]
\end{definition}

To understand this term it helps to consider its type one component at a time and correspond that to the term definition. $f$ is the only parameter of inj and has the type $\underline{n}^{m+1} \Rightarrow \underline{0}$, meaning $f$ is a function mapping $\underline{n}^{m+1}$, an $m+1$ fold product of numerals $\le$ n, to the empty type $\underline{0}$.  The second half of inj's type is a pair with the first element being $\underline{m+1}$, precisely the type provided by 'catch $f$' and a tuple containing the same number of candidate continuation functions as there are numerals in the language. Each continuation has the type $\underline{n}^{m} \Rightarrow \underline{0}$ which can be seen as the same type as $f$ if it took one fewer argument. Looking at the term, it is clear that this is because an argument is applied in the strict position as part of the continuation. 

This action effectively represents bounded terms as a series of nested tuples consisting of the index at which a function is strict and a tuple of continuation functions for all the possible values at its strict argument. 

\subsubsection{Projection from tuple form}
\begin{definition}
    The function \emph{proj}, which transforms a term in tuple form back to its original type, is definable as \\
    proj: $\llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \rrbracket \rightarrow \llbracket \underline{n}^{m+1} \Rightarrow \underline{0} \rrbracket$\\\\
    proj($\langle i, c\rangle$)($x$) =
    $\begin{cases}
        i\ &\text{if}\ i \in \{\top, \bot\}\\
        \top\ &\text{if}\ \pi_i(x) = \top, \text{ or } \pi_i(x) = \top\\
        \top\ &\text{if}\ f'_i(\lceil x \rceil_i) = \top \text{, where } f'_i = \pi_j(c)\\
        \bot\ &\text{otherwise}
    \end{cases}$\\\\
    This is definable as the term
    \[
    \lambda tx. \text{case } \langle \pi_1(t), \langle \text{case } \langle \pi_i(x), \langle \pi_j(\pi_2(t)) \lceil \langle \pi_0(x), \dots, \pi_m(x)\rangle\rceil_j\ | \ j \le n \rangle \rangle\ |\ i \leq m \rangle \rangle
    \]
\end{definition}

To understand this term, it helps to bear in mind the structure of $f$ when in tuple form. The parameter $t$ is of the form $\langle i, c\rangle$ where $i$ is the index at which $f$ is strict and $c$ is a tuple of continuation functions for each possible value supplied at the strict position. The parameter $x$ is the tuple of $f$'s parameters. In that sense the term defined here is a curried function of the type $\llbracket \underline{m + 1} \times (\underline{n}^{m} \Rightarrow \underline{0})^n \times \underline{n}^{m+1} \rrbracket \rightarrow \llbracket \underline{0} \rrbracket$.

\subsection{Implementation details}
The definitions of inj and proj are quite dense and require many prerequisite features for their implementation. In Chapter \ref{section: language representation} we discuss most of these, however, it is worth noting some subtleties which may not be obvious:
\begin{enumerate}
    \item Care is needed when defining if function arguments and tuples are 0-indexed or not. Both terms rely on the definition of catch to index a tuple, so their indexing must be consistent.
    \item Extra care is required when choosing a representation for tuples. There are two cases in the definitions of inj and proj, the n-fold product and the binary product. 
    \item The construction of each term depends on the type of their input, thus a suitable method of type inference is required. This influenced the decision to rely on type inference for type checking discussed in Chapter \ref{section: typing}.
\end{enumerate}

To expand on point (2), recall that the $n$-fold is a product with length $n$ where each element is of the same type and that the binary product has no such restriction on typing. This should be clear by the fact that inj returns a product of two elements with different types, a numeral and another product. It follows that it is not possible to define this pairing as an n-fold product and have the program correctly typed.

One solution is to construct all n-fold products from the repeated application of binary products, however, having a representation for n-fold products relying on an underlying list is very convenient and provides many operations out of the box. The alternative is to include definitions for both kinds of product including $\pi_1 \text{ and } \pi_2$ terms for the binary case. This enables the type to be correctly and statically determined. 

\subsection{Affinely definable retraction on types}
\begin{definition}
    A \emph{retraction} $X \trianglelefteq Y$ is a function $f: Y \rightarrow X$ where there exists a function $g: X \rightarrow Y$ such that $g \circ f = id_x$. 
    
    A retraction is in some senses a weaker form of bijection, where elements from $X$ can be retrieved from $Y$ after an initial mapping without information loss, but without the implication of the function being invertible.
\end{definition}

For types $\tau$ and $\sigma$ in SPCF (and bounded SPCF) the retraction $\tau \trianglelefteq \sigma$ is ASPCF defineable by $\text{inj}: \tau \rightarrow \sigma$ and $\text{proj}: \sigma \rightarrow \tau$. For example $\text{proj}(\text{inj}(f))= f$.

If $\tau_1 \trianglelefteq \tau_2$ and $\sigma_1 \trianglelefteq \sigma_2$, then $\tau_1 \times \sigma_1 \trianglelefteq \tau_2 \times \sigma_2$ and $\tau_1 \Rightarrow \sigma_1 \trianglelefteq \tau_2 \Rightarrow \sigma_2 $.

Speak about the universal type, how it has to be affinely typeable to gain this desired effect.

There is a flip-flop correspondence between the size of the term and the size of the type.

Lair described a potential usecase to be a codebase which favours accuracy over size and this can be used to be more readable, I think it comprehensively is less clear. but further research could be done.

\section{Transforming unbounded terms}
The transformation for unbounded terms can be seen in much the same way as for bounded terms. The idea is to construct a retract $(o \rightarrow o) \rightarrow \underline{0} \trianglelefteq o \rightarrow o$ into a lower order type, and inductively apply it to 'reduce' a term's type until it is a universal type $o \rightarrow o$. It can be thought of as a direct extension of the process described in Section \ref{section:inj-proj}, but it must use a different representation rather than finite tuples, and this, in turn, creates some more machinery to handle. This is because to adapt the terms from Section \ref{section:inj-proj} directly for unbounded terms would require fixed point recursion, which is not affinely typeable.

While this transformation \emph{can} be viewed as an extension of the bounded case, it may also help to view it from the point of view of a two-player game. Specifically, seeing as functions may be represented as decision trees, its traversal may be represented as a game \emph{strategy} $\sigma : (\textbf{N} \times \textbf{N})^* \rightarrow \textbf{N}_\bot^\top$, dictating the responses a player with a finite list of pairs of moves. Put differently, it can represent the application of a function $f$ to its argument $g \in \textbf{N}\rightarrow \textbf{N}_\bot^\top$ by sequentially choosing a number $n$ and evaluating the second player's response $g(n)$ until the game is stopped by either player choosing $\top$ or $\bot$.

A function $f : (\textbf{N} \rightarrow \textbf{N}_\bot^\top)$ strategy is definable as strat$(f)(s) = \text{catch}(\lambda e.f(e[s]))$.

\begin{definition}
    \emph{label} is a non-local control operator similar to catch. However, it returns the value of a function's strict argument rather than its index. It is directly definable in terms of catch and is in some senses a weaker version of the \emph{call/cc} function defined by Cartwright and Felleisen. An informal example: label $\lambda k.(k N) = k$.
\end{definition}
\begin{definition}
    Sequential \emph{composition} $(M^o; N^{o \rightarrow \tau}): \tau$ is identical to term application but with the guarantee to always be affinely typable.
\end{definition}
\begin{definition}
    \emph{Iteration} is represented by a constant \emph{it}: $(o \rightarrow o) \rightarrow o \rightarrow \underline{0}$ and is representable as the term $\lambda f. Y \lambda g. \lambda x. (f;x)g$.
\end{definition}
\begin{definition}
    An surjective partial pairing constant \emph{pair}: $o \rightarrow o \rightarrow o$ is definable as pair(m)(n) = $2^m(2n + 1)$ with accessors definable such that \emph{fst}$(2^m(2n + 1)) = m$ and \emph{snd}$(2^m(2n + 1)) = n$.

    The term $N; (pair M)$ is written as $M * N$ and finite lists of $k$ natural numbers may be represented as the numeral $n_1 * (n_2 * \dots * (n_k * 0))$.
\end{definition}

The surjective pairing is the star of this transformation, it quite remarkably uses the fundamental theorem of arithmetic to represent pairs of numbers as a unique number. This, along with \emph{label} and favouring iteration over recursion, is what enables the modelling of all partial recursive functions in ASPCF. This is important because the argument is made that ASPCF can model precisely the same functions described by Turning machines, hence it is fully expressive. It is used to replace the explicit product type used in Section \ref{section:inj-proj}.

\begin{definition}
\[
\begin{split}
    \text{seq}_i(k, e)& \in ((\mathbf{N} \times \mathbf{N})*)\\
    \text{seq}_0(k, e)& = \epsilon \\
    \text{seq}_{n+1}(k, e)& = 
    \begin{cases}
        \top \text{ if seq}_n(k,e) = \top\\
        \top \text{ if seq}_n(k,e) = s \text{ and }k(s) = \top\\
        \top \text{ if seq}_n(k,e) = s \text{ and }k(s) = i \text{ and } e(i) = \top\\
        s(i, j) \text{ if seq}_n(k,e) = s \text{ and }k(s) = i \text{ and } e(i) = j\\
        \bot \text{ otherwise }
    \end{cases}
\end{split}
\]
\end{definition}

\begin{definition}
    \[
    \text{fun}(k)(e)=
    \begin{cases}
        \top \text{ if } seq_n(k,e) = \top \quad (\exists n)\\
        \bot \text{ otherwise}
    \end{cases}
    \]
    Where fun(strat($f$)) = $f$.
\end{definition}

\begin{definition}
    inj is definable as the term
    \[
    \lambda f. \lambda x. \text{ label} \lambda k. k((\text{strat}(\lambda y.k((f y) * 1))x)*0)
    \]
\end{definition}

\begin{definition}
    proj is definable as the term
    \[
    \lambda g. \lambda h. \text{ label} \lambda k. \text{fun} (\lambda x.\text{If0}(\text{snd}(x))\ \text{then}\ (k\ \text{fst}(x)) \text{ else } g\ \text{fst}(x))
    \]
\end{definition}

The internal conditional statement can be seen as an encoding for the same case conditional used in Section \ref{section:inj-proj} for this new representation of lists as numerals.


\begin{definition}
    Equality $(M = N):o$ returns 0 if M and N evaluate to the same numeral and 1 otherwise. 
\end{definition}

\begin{definition}
    strat = $\lambda f. \lambda y. \text{ label }\lambda g.f(\lambda x \text{ label } \lambda k (\text{sub}^* y))$ where sub$(g, k, x) = \lambda v. \text{ If0 } v \text{ then } k(g\ x) \text{ else } (\text{If0 } (\text{fst}(v) = x) \text{ then } k\ \text{fst}(\text{snd } v) \text{ else snd } v )$
\end{definition}

\subsection{Implementation notes}
We do not provide a working implementation of the affine transformation for unbounded terms, however, we have some observations which may be of interest:  

\begin{enumerate}
    \item The surjective pairing is sound but rather awkward to implement. It is tempting to use an intermediate representation, for example, two terms which work together to represent a partially applied pair and a fully applied pair, however, this raises further oddities when attempting to infer its type, making a naive implementation more appealing. Ironically, it would probably be easier to construct this term with a low-level language like C, where handling numbers as concatenations of other numbers is more idiomatic than in a strict functional language like Haskell.
    
    \item While \emph{label} is conceptually similar to catch, it is easiest to implement as a separate construct. Operationally speaking, it is easier to consider it as another form of tree traversal than encode it in terms of catch as suggested originally by Felleisen.

    \item An interesting point of exploration is the attempt to naively implement the transformation for bounded terms using Haskell's lazy lists, which allow the ergonomic creation and mutation of infinite sequences. This would ignore the point of making the transformation affinely typeable but has the potential to achieve the same results if that property was not required. 
\end{enumerate}

\section{Conclusion}
We thoroughly discuss the denesting action, its roots in CPS programming, and various insights from the implementation process. We provide a real-world implementation of the transformation on bounded terms and discuss areas of exploration for unbounded terms.

\chapter{Results}
why is this better?
is it more readable?
some comments here even if brief would create a nice conclusion

How much it grows. Example term with differing upper bound. Then different type.

a graph of growth of the term (it gets very large) with type size and upper bound size.

a graph on runtime to perform the transformation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}



This is the chapter in which you review the major achievements in the light of your original objectives, critique the process, critique your own learning and identify possible future work.

highlight novelties.

\vfill
\section{Word Count}
The number of words until this point, excluding front matter: XXX.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\chapter{Code Repository Link}
\url{https://github.com/jayrabjohns/dissertation-refactoring-spcf}

%%
%% Use the appendix for major chunks of detailed work, such as these. Tailor
%% these to your own requirements
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Design Diagrams}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{User Documentation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Raw Results Output}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Code}

\section{Lexer}\label{appendix: lexer}
First macros are defined for digits and string characters. Then list of token patterns is defined using those macros. Then an algebraic datatype is used to enumerate all possible tokens, this is primarily what will be used later in the parser. Finally, some helper functions are defined to handle errors during lexing and for the handling of the previously defined ADT.

\lstinputlisting[basicstyle=\scriptsize]{Lexer.x}

\section{Parser}\label{appendix: parser}
The same set of tokens defined in the lexer are brought forward to the definition of our parser. It's this compatibility between Happy and Alex that makes them so popular. Then a series of rules are defined for how to inductively construct the AST from the defined tokens. Finally, some helper functions are defined enabling our program to interface with the parser later. 

\lstinputlisting[basicstyle=\scriptsize]{Parser.y}

%% NOTE For this to typeset correctly, ensure you use the pdflatex
%%      command in preference to the latex command.  If you do not have
%%      the pdflatex command, you will need to remove the landscape and
%%      multicols tags and just make do with single column listing output

%\begin{landscape}
%\begin{multicols}{2}
%\section{File: yourCodeFile.java}
%\lstinputlisting[basicstyle=\scriptsize]{yourCodeFile.java}
%\end{multicols}
%\end{landscape}

\chapter{Example usage of injection and projection terms}

\begin{lstlisting}[mathescape, caption={Interpreter logs from applying injection and projection terms. The upper bound on values n = 2, effectively representing the booleans.}]
f = \p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>

proj(inj(f)) = 
\b:(NatxNat) => 
  Case <
    $\pi$1(Case <Catch (\p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>), 
        <<0, <(\a:Nat => Case <Case <1, <0, (Case <0, a>)>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <1, (Case <0, a>)>>, <$\bot$, $\bot$>>)>>,
        <1, <(\a:Nat => Case <Case <1, <(Case <0, a>), 0>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <(Case <0, a>), 1>>, <$\bot$, $\bot$>>)>>>>
      ),
    <(Case <Case <0, b>,
      <((Case <0, $\pi$2(Case <Catch (\p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>), 
          <<0, <(\a:Nat => Case <Case <1, <0, (Case <0, a>)>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <1, (Case <0, a>)>>, <$\bot$, $\bot$>>)>>, 
          <1, <(\a:Nat => Case <Case <1, <(Case <0, a>), 0>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <(Case <0, a>), 1>>, <$\bot$, $\bot$>>)>>>>)>
        ) <(Case <1, b>)>
        ), 
        ((Case <1, $\pi$2(Case <Catch (\p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>),
          <<0, <(\a:Nat => Case <Case <1, <0, (Case <0, a>)>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <1, (Case <0, a>)>>, <$\bot$, $\bot$>>)>>,
          <1, <(\a:Nat => Case <Case <1, <(Case <0, a>), 0>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <(Case <0, a>), 1>>, <$\bot$, $\bot$>>)>>>>)>
        ) <(Case <1, b>)>
        )>>
    ),
    (Case <Case <1, b>,
      <((Case <0, $\pi$2(Case <Catch (\p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>),
          <<0, <(\a:Nat => Case <Case <1, <0, (Case <0, a>)>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <1, (Case <0, a>)>>, <$\bot$, $\bot$>>)>>,
          <1, <(\a:Nat => Case <Case <1, <(Case <0, a>), 0>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <(Case <0, a>), 1>>, <$\bot$, $\bot$>>)>>>>)>
        ) <(Case <0, b>)>
        ),
        ((Case <1, $\pi$2(Case <Catch (\p:(NatxNat) => Case <Case <1, p>, <$\bot$, $\bot$>>),
          <<0, <(\a:Nat => Case <Case <1, <0, (Case <0, a>)>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <1, (Case <0, a>)>>, <$\bot$, $\bot$>>)>>,
          <1, <(\a:Nat => Case <Case <1, <(Case <0, a>), 0>>, <$\bot$, $\bot$>>), (\a:Nat => Case <Case <1, <(Case <0, a>), 1>>, <$\bot$, $\bot$>>)>>>>)>
        ) <(Case <0, b>)>
        )>>
    )>>

\end{lstlisting}

\chapter{Example program execution}\label{appendix: term evaluations}
\subsection{Program Definition}

\captionof{listing}{A program written in our implementation of SPCF consisting of multiple term definitions and evaluations\label{listing:example-program-appendix}}
\begin{minted}[breaklines]{haskell}
addLeftTerm = \f:Nat->Nat->Nat => \x:Nat => \y:Nat => if0 x then y else (succ (f (pred x) y));
add = \x:Nat => \y:Nat => (fix addLeftTerm) x y;

mulTerm = \f:Nat->Nat->Nat => 
          \x:Nat => 
          \y:Nat => 
            if0 y 
              then x 
              else (add x (f x (pred y)));

mul = \x: Nat => \y:Nat => (fix mulTerm) x (pred y);

factorial = \f: Nat->Nat => \n:Nat => if0 n then 1 else (mul n (f (pred n)));
fact = \n: Nat => (fix factorial) n;

eval (fact 5);

addLeftTerm = \f:Nat->Nat->Nat => 
              \x:Nat => 
              \y:Nat => if0 x then y else (succ (f (pred x) y));
add = \x:Nat => \y:Nat => (fix addLeftTerm) x y;

eval (add error1 error2);

addRightTerm = \f:Nat->Nat->Nat => \x:Nat => \y:Nat => if0 y then x else (succ (f x (pred y)));
add' = \x:Nat => \y:Nat => (fix addRightTerm) x y;
eval (add' error1 error2);
\end{minted}

\subsection{Program Type Judgements}
\captionof{listing}{Interpreter logs for typing the program in \ref{listing:example-program-appendix}}
\begin{minted}[breaklines]{haskell}
Type judgement for addLeftTerm = \f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))
[f]: o->o->o
[x]: o
[y]: o
[pred x]: o
[f (pred x)]: o->o
[f (pred x) y]: o
[succ (f (pred x) y)]: o
[if0 x then y else (succ (f (pred x) y))]: o
[\y:o => if0 x then y else (succ (f (pred x) y))]: o->o
[\x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 x then y else (succ (f (pred x) y))]: (o->o->o)->o->o->o

Type judgement for addRightTerm = \f:o->o->o => \x:o => \y:o => if0 y then x else (succ (f x (pred y)))
[f]: o->o->o
[x]: o
[y]: o
[pred y]: o
[f x]: o->o
[f x (pred y)]: o
[succ (f x (pred y))]: o
[if0 y then x else (succ (f x (pred y)))]: o
[\y:o => if0 y then x else (succ (f x (pred y)))]: o->o
[\x:o => \y:o => if0 y then x else (succ (f x (pred y)))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 y then x else (succ (f x (pred y)))]: (o->o->o)->o->o->o

Type judgement for add = \x:o => \y:o => (fix addLeftTerm) x y
[x]: o
[y]: o
[fix addLeftTerm]: o->o->o
[(fix addLeftTerm) x]: o->o
[(fix addLeftTerm) x y]: o
[\y:o => (fix addLeftTerm) x y]: o->o
[\x:o => \y:o => (fix addLeftTerm) x y]: o->o->o

Type judgement for add' = \x:o => \y:o => (fix addRightTerm) x y
[x]: o
[y]: o
[fix addRightTerm]: o->o->o
[(fix addRightTerm) x]: o->o
[(fix addRightTerm) x y]: o
[\y:o => (fix addRightTerm) x y]: o->o
[\x:o => \y:o => (fix addRightTerm) x y]: o->o->o

Type judgement for mulTerm = \f:o->o->o => \x:o => \y:o => if0 y then x else add x (f x (pred y))
[f]: o->o->o
[x]: o
[y]: o
[pred y]: o
[f x]: o->o
[f x (pred y)]: o
[add x]: o->o
[add x (f x (pred y))]: o
[if0 y then x else add x (f x (pred y))]: o
[\y:o => if0 y then x else add x (f x (pred y))]: o->o
[\x:o => \y:o => if0 y then x else add x (f x (pred y))]: o->o->o
[\f:o->o->o => \x:o => \y:o => if0 y then x else add x (f x (pred y))]: (o->o->o)->o->o->o

Type judgement for mul = \x:o => \y:o => (fix mulTerm) x (pred y)
[x]: o
[y]: o
[pred y]: o
[fix mulTerm]: o->o->o
[(fix mulTerm) x]: o->o
[(fix mulTerm) x (pred y)]: o
[\y:o => (fix mulTerm) x (pred y)]: o->o
[\x:o => \y:o => (fix mulTerm) x (pred y)]: o->o->o

Type judgement for factorial = \f:o->o => \n:o => if0 n then 1 else mul n (f (pred n))
[f]: o->o
[n]: o
[1]: o
[pred n]: o
[f (pred n)]: o
[mul n]: o->o
[mul n (f (pred n))]: o
[if0 n then 1 else mul n (f (pred n))]: o
[\n:o => if0 n then 1 else mul n (f (pred n))]: o->o
[\f:o->o => \n:o => if0 n then 1 else mul n (f (pred n))]: (o->o)->o->o

Type judgement for fact = \n:o => (fix factorial) n
[n]: o
[fix factorial]: o->o
[(fix factorial) n]: o
[\n:o => (fix factorial) n]: o->o

Type judgement for x = 5
[5]: o

Type judgement for y = 3
[3]: o

Type judgement for eval {add x y}
[add x]: o->o
[add x y]: o

Type judgement for eval {catch add'}
[catch add']: o

Type judgement for eval {mul x y}
[mul x]: o->o
[mul x y]: o

Type judgement for eval {fact 5}
[5]: o
[fact 5]: o
\end{minted}

\subsection{Program Evaluation}

\end{document}
